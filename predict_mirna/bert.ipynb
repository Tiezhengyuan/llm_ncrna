{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "32147ccd-18aa-4ebe-9955-cd4a140540dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# reload all changed moduels\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "24c193c9-6f7e-40da-a4df-11420287e2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_data import MyData\n",
    "from my_collate import MyCollate\n",
    "from my_embedding import MyEmbedding\n",
    "from my_process import MyProcess\n",
    "from my_parser import MyParser\n",
    "from model_bert import ModelBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b746b363-9d42-45fa-ac55-2c3342dcb42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of origin seq: 48882\n",
      "Number of trim seq: 683174\n",
      "Number of shuffle seq: 48882\n",
      "Number of 5srrnadb.fasta: 11415\n",
      "Number of gtrnadb.fasta: 236835\n",
      "Number of pirbase.fasta: 219278\n",
      "Total number of data: 1146682\n",
      "Observe data: ('Caenorhabditis elegans', 'TGAGGTAGTAGGTTGTATAGTT')\n"
     ]
    }
   ],
   "source": [
    "# prepare dataset\n",
    "md = MyData()\n",
    "train_data = md.get_full_data()\n",
    "print('Total number of data:', len(train_data))\n",
    "print('Observe data:', train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f062becc-60be-4b0e-9825-a5fd6acf4d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('other', 'TNGGCAGCGTGGTTCCTGTTGGTGAGCTCT')\n",
      "917346 <class 'torch.utils.data.dataset.Subset'>\n",
      "0 1\n"
     ]
    }
   ],
   "source": [
    "me = MyEmbedding(train_data)\n",
    "train_dataset, valid_dataset = me.split()\n",
    "\n",
    "train_texts = [i[1] for i in train_dataset]\n",
    "train_labels = [0 if i[0] in ('shuffle', 'random', 'other') else 1 for i in train_dataset]\n",
    "\n",
    "valid_texts = [i[1] for i in valid_dataset]\n",
    "valid_labels = [0 if i[0] in ('shuffle', 'random', 'other') else 1 for i in valid_dataset]\n",
    "\n",
    "print(train_labels[0], valid_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "20c583ab-ffcd-47c6-8a97-054e73bbd65a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertTokenizerFast(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e7439e19-83cd-4a7e-90cf-e1e86a98fcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True)\n",
    "valid_encodings = tokenizer(list(valid_texts), truncation=True, padding=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "36301150-9fed-405e-b8d4-4c64246dca5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=57, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "652c1032-2db0-4f73-8b6d-6d42c81658bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "train_dataset = MyDataset(train_encodings, train_labels)\n",
    "valid_dataset = MyDataset(valid_encodings, valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2d45ce14-eab9-473f-97f1-c9483b676d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "22733a1a-a8ba-45e8-81a6-17f8f0927483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1056, 13512,  6593, 18195,  9468, 18195,  4017, 18195,  6593,\n",
       "          18195,  2278,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0],\n",
       "         [  101, 22975, 19629, 13512,  9468,  2102, 18195,  5946, 13871,  2102,\n",
       "           3654,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0],\n",
       "         [  101, 11937, 11057,  8490,  3540,  8490,  3540, 11057,  3540, 11057,\n",
       "          11057,  4779,  9468,  2278, 13871,  2696,  2278,   102,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0],\n",
       "         [  101,  1056, 23033, 16098, 25572, 13512, 20697,  4779,  2102,   102,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0],\n",
       "         [  101, 10507,  6305, 18195,  6305,  3540, 13871, 13512, 11057,  3654,\n",
       "            102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0],\n",
       "         [  101,  1039, 18195,  4779, 13871,  2102, 18195, 16098, 13535, 13871,\n",
       "           3654,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0],\n",
       "         [  101,  1043, 20697,  6593, 23033, 13871, 11266,  6593, 13871,  2290,\n",
       "            102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0],\n",
       "         [  101,  1043, 18195, 11057,  3540,  4017,  9468, 11266, 13535,  4779,\n",
       "           6593,  2278,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'labels': tensor([1, 1, 0, 1, 1, 1, 1, 1])}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "40bc5eda-571a-49a8-a3d7-c56f774a2386",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715a4cdc-8637-412a-b18e-e0e6dda1eefd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1dba8db5-c380-465f-8ce0-640122275189",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, data_loader, device):\n",
    "    with torch.no_grad():\n",
    "        correct_pred, num_examples = 0, 0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "        \n",
    "        ### Prepare data\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs['logits']\n",
    "            predicted_labels = torch.argmax(logits, 1)\n",
    "            num_examples += labels.size(0)\n",
    "            correct_pred += (predicted_labels == labels).sum()\n",
    "        \n",
    "        return correct_pred.float()/num_examples * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fc3e72c7-39f3-43f6-8cbc-54829981fd7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001/0003 | Batch 0000/114669 | Loss: 0.6973\n",
      "Epoch: 0001/0003 | Batch 0250/114669 | Loss: 0.0566\n",
      "Epoch: 0001/0003 | Batch 0500/114669 | Loss: 0.7737\n",
      "Epoch: 0001/0003 | Batch 0750/114669 | Loss: 0.0361\n",
      "Epoch: 0001/0003 | Batch 1000/114669 | Loss: 0.0278\n",
      "Epoch: 0001/0003 | Batch 1250/114669 | Loss: 0.0405\n",
      "Epoch: 0001/0003 | Batch 1500/114669 | Loss: 0.3690\n",
      "Epoch: 0001/0003 | Batch 1750/114669 | Loss: 0.0416\n",
      "Epoch: 0001/0003 | Batch 2000/114669 | Loss: 0.0383\n",
      "Epoch: 0001/0003 | Batch 2250/114669 | Loss: 0.0205\n",
      "Epoch: 0001/0003 | Batch 2500/114669 | Loss: 0.0491\n",
      "Epoch: 0001/0003 | Batch 2750/114669 | Loss: 0.4311\n",
      "Epoch: 0001/0003 | Batch 3000/114669 | Loss: 0.0934\n",
      "Epoch: 0001/0003 | Batch 3250/114669 | Loss: 0.3605\n",
      "Epoch: 0001/0003 | Batch 3500/114669 | Loss: 0.6571\n",
      "Epoch: 0001/0003 | Batch 3750/114669 | Loss: 0.0362\n",
      "Epoch: 0001/0003 | Batch 4000/114669 | Loss: 0.0430\n",
      "Epoch: 0001/0003 | Batch 4250/114669 | Loss: 0.0526\n",
      "Epoch: 0001/0003 | Batch 4500/114669 | Loss: 0.8403\n",
      "Epoch: 0001/0003 | Batch 4750/114669 | Loss: 0.8276\n",
      "Epoch: 0001/0003 | Batch 5000/114669 | Loss: 0.6648\n",
      "Epoch: 0001/0003 | Batch 5250/114669 | Loss: 0.5700\n",
      "Epoch: 0001/0003 | Batch 5500/114669 | Loss: 0.5751\n",
      "Epoch: 0001/0003 | Batch 5750/114669 | Loss: 0.5520\n",
      "Epoch: 0001/0003 | Batch 6000/114669 | Loss: 0.7925\n",
      "Epoch: 0001/0003 | Batch 6250/114669 | Loss: 0.6003\n",
      "Epoch: 0001/0003 | Batch 6500/114669 | Loss: 0.5798\n",
      "Epoch: 0001/0003 | Batch 6750/114669 | Loss: 0.6744\n",
      "Epoch: 0001/0003 | Batch 7000/114669 | Loss: 0.4418\n",
      "Epoch: 0001/0003 | Batch 7250/114669 | Loss: 0.4371\n",
      "Epoch: 0001/0003 | Batch 7500/114669 | Loss: 0.6780\n",
      "Epoch: 0001/0003 | Batch 7750/114669 | Loss: 0.9286\n",
      "Epoch: 0001/0003 | Batch 8000/114669 | Loss: 0.5676\n",
      "Epoch: 0001/0003 | Batch 8250/114669 | Loss: 0.5903\n",
      "Epoch: 0001/0003 | Batch 8500/114669 | Loss: 0.7326\n",
      "Epoch: 0001/0003 | Batch 8750/114669 | Loss: 0.6910\n",
      "Epoch: 0001/0003 | Batch 9000/114669 | Loss: 0.8451\n",
      "Epoch: 0001/0003 | Batch 9250/114669 | Loss: 0.6632\n",
      "Epoch: 0001/0003 | Batch 9500/114669 | Loss: 0.7692\n",
      "Epoch: 0001/0003 | Batch 9750/114669 | Loss: 0.5856\n",
      "Epoch: 0001/0003 | Batch 10000/114669 | Loss: 0.5535\n",
      "Epoch: 0001/0003 | Batch 10250/114669 | Loss: 0.5856\n",
      "Epoch: 0001/0003 | Batch 10500/114669 | Loss: 0.7851\n",
      "Epoch: 0001/0003 | Batch 10750/114669 | Loss: 0.5874\n",
      "Epoch: 0001/0003 | Batch 11000/114669 | Loss: 0.5694\n",
      "Epoch: 0001/0003 | Batch 11250/114669 | Loss: 0.6616\n",
      "Epoch: 0001/0003 | Batch 11500/114669 | Loss: 0.3702\n",
      "Epoch: 0001/0003 | Batch 11750/114669 | Loss: 0.5867\n",
      "Epoch: 0001/0003 | Batch 12000/114669 | Loss: 0.5460\n",
      "Epoch: 0001/0003 | Batch 12250/114669 | Loss: 0.4870\n",
      "Epoch: 0001/0003 | Batch 12500/114669 | Loss: 0.4944\n",
      "Epoch: 0001/0003 | Batch 12750/114669 | Loss: 0.5773\n",
      "Epoch: 0001/0003 | Batch 13000/114669 | Loss: 0.5697\n",
      "Epoch: 0001/0003 | Batch 13250/114669 | Loss: 0.7519\n",
      "Epoch: 0001/0003 | Batch 13500/114669 | Loss: 0.7462\n",
      "Epoch: 0001/0003 | Batch 13750/114669 | Loss: 0.7714\n",
      "Epoch: 0001/0003 | Batch 14000/114669 | Loss: 0.6598\n",
      "Epoch: 0001/0003 | Batch 14250/114669 | Loss: 0.4904\n",
      "Epoch: 0001/0003 | Batch 14500/114669 | Loss: 0.5633\n",
      "Epoch: 0001/0003 | Batch 14750/114669 | Loss: 0.8996\n",
      "Epoch: 0001/0003 | Batch 15000/114669 | Loss: 0.6789\n",
      "Epoch: 0001/0003 | Batch 15250/114669 | Loss: 0.5626\n",
      "Epoch: 0001/0003 | Batch 15500/114669 | Loss: 0.5676\n",
      "Epoch: 0001/0003 | Batch 15750/114669 | Loss: 0.5945\n",
      "Epoch: 0001/0003 | Batch 16000/114669 | Loss: 0.6668\n",
      "Epoch: 0001/0003 | Batch 16250/114669 | Loss: 0.5076\n",
      "Epoch: 0001/0003 | Batch 16500/114669 | Loss: 0.5714\n",
      "Epoch: 0001/0003 | Batch 16750/114669 | Loss: 0.8729\n",
      "Epoch: 0001/0003 | Batch 17000/114669 | Loss: 0.6638\n",
      "Epoch: 0001/0003 | Batch 17250/114669 | Loss: 0.6677\n",
      "Epoch: 0001/0003 | Batch 17500/114669 | Loss: 0.6606\n",
      "Epoch: 0001/0003 | Batch 17750/114669 | Loss: 0.5822\n",
      "Epoch: 0001/0003 | Batch 18000/114669 | Loss: 0.8554\n",
      "Epoch: 0001/0003 | Batch 18250/114669 | Loss: 0.7729\n",
      "Epoch: 0001/0003 | Batch 18500/114669 | Loss: 0.6632\n",
      "Epoch: 0001/0003 | Batch 18750/114669 | Loss: 0.2367\n",
      "Epoch: 0001/0003 | Batch 19000/114669 | Loss: 0.0927\n",
      "Epoch: 0001/0003 | Batch 19250/114669 | Loss: 0.0905\n",
      "Epoch: 0001/0003 | Batch 19500/114669 | Loss: 0.3715\n",
      "Epoch: 0001/0003 | Batch 19750/114669 | Loss: 0.1280\n",
      "Epoch: 0001/0003 | Batch 20000/114669 | Loss: 0.3203\n",
      "Epoch: 0001/0003 | Batch 20250/114669 | Loss: 0.0597\n",
      "Epoch: 0001/0003 | Batch 20500/114669 | Loss: 0.5679\n",
      "Epoch: 0001/0003 | Batch 20750/114669 | Loss: 0.1286\n",
      "Epoch: 0001/0003 | Batch 21000/114669 | Loss: 0.0920\n",
      "Epoch: 0001/0003 | Batch 21250/114669 | Loss: 0.3906\n",
      "Epoch: 0001/0003 | Batch 21500/114669 | Loss: 0.1822\n",
      "Epoch: 0001/0003 | Batch 21750/114669 | Loss: 0.3422\n",
      "Epoch: 0001/0003 | Batch 22000/114669 | Loss: 0.6685\n",
      "Epoch: 0001/0003 | Batch 22250/114669 | Loss: 0.5010\n",
      "Epoch: 0001/0003 | Batch 22500/114669 | Loss: 0.4188\n",
      "Epoch: 0001/0003 | Batch 22750/114669 | Loss: 0.4860\n",
      "Epoch: 0001/0003 | Batch 23000/114669 | Loss: 0.6669\n",
      "Epoch: 0001/0003 | Batch 23250/114669 | Loss: 0.4637\n",
      "Epoch: 0001/0003 | Batch 23500/114669 | Loss: 0.5045\n",
      "Epoch: 0001/0003 | Batch 23750/114669 | Loss: 0.4691\n",
      "Epoch: 0001/0003 | Batch 24000/114669 | Loss: 0.4737\n",
      "Epoch: 0001/0003 | Batch 24250/114669 | Loss: 0.9357\n",
      "Epoch: 0001/0003 | Batch 24500/114669 | Loss: 0.6569\n",
      "Epoch: 0001/0003 | Batch 24750/114669 | Loss: 0.7040\n",
      "Epoch: 0001/0003 | Batch 25000/114669 | Loss: 0.9315\n",
      "Epoch: 0001/0003 | Batch 25250/114669 | Loss: 0.5829\n",
      "Epoch: 0001/0003 | Batch 25500/114669 | Loss: 0.5683\n",
      "Epoch: 0001/0003 | Batch 25750/114669 | Loss: 0.8291\n",
      "Epoch: 0001/0003 | Batch 26000/114669 | Loss: 0.6684\n",
      "Epoch: 0001/0003 | Batch 26250/114669 | Loss: 0.5132\n",
      "Epoch: 0001/0003 | Batch 26500/114669 | Loss: 0.6913\n",
      "Epoch: 0001/0003 | Batch 26750/114669 | Loss: 0.5871\n",
      "Epoch: 0001/0003 | Batch 27000/114669 | Loss: 0.4739\n",
      "Epoch: 0001/0003 | Batch 27250/114669 | Loss: 0.4814\n",
      "Epoch: 0001/0003 | Batch 27500/114669 | Loss: 0.5866\n",
      "Epoch: 0001/0003 | Batch 27750/114669 | Loss: 0.5238\n",
      "Epoch: 0001/0003 | Batch 28000/114669 | Loss: 0.4752\n",
      "Epoch: 0001/0003 | Batch 28250/114669 | Loss: 0.6807\n",
      "Epoch: 0001/0003 | Batch 28500/114669 | Loss: 0.6852\n",
      "Epoch: 0001/0003 | Batch 28750/114669 | Loss: 0.5900\n",
      "Epoch: 0001/0003 | Batch 29000/114669 | Loss: 0.7941\n",
      "Epoch: 0001/0003 | Batch 29250/114669 | Loss: 0.6653\n",
      "Epoch: 0001/0003 | Batch 29500/114669 | Loss: 0.5740\n",
      "Epoch: 0001/0003 | Batch 29750/114669 | Loss: 0.4840\n",
      "Epoch: 0001/0003 | Batch 30000/114669 | Loss: 0.5725\n",
      "Epoch: 0001/0003 | Batch 30250/114669 | Loss: 0.4238\n",
      "Epoch: 0001/0003 | Batch 30500/114669 | Loss: 0.5895\n",
      "Epoch: 0001/0003 | Batch 30750/114669 | Loss: 0.5972\n",
      "Epoch: 0001/0003 | Batch 31000/114669 | Loss: 0.6779\n",
      "Epoch: 0001/0003 | Batch 31250/114669 | Loss: 0.6733\n",
      "Epoch: 0001/0003 | Batch 31500/114669 | Loss: 0.5018\n",
      "Epoch: 0001/0003 | Batch 31750/114669 | Loss: 0.5836\n",
      "Epoch: 0001/0003 | Batch 32000/114669 | Loss: 0.4964\n",
      "Epoch: 0001/0003 | Batch 32250/114669 | Loss: 0.5349\n",
      "Epoch: 0001/0003 | Batch 32500/114669 | Loss: 0.5582\n",
      "Epoch: 0001/0003 | Batch 32750/114669 | Loss: 0.5980\n",
      "Epoch: 0001/0003 | Batch 33000/114669 | Loss: 0.5149\n",
      "Epoch: 0001/0003 | Batch 33250/114669 | Loss: 0.5483\n",
      "Epoch: 0001/0003 | Batch 33500/114669 | Loss: 0.6662\n",
      "Epoch: 0001/0003 | Batch 33750/114669 | Loss: 0.5035\n",
      "Epoch: 0001/0003 | Batch 34000/114669 | Loss: 0.5822\n",
      "Epoch: 0001/0003 | Batch 34250/114669 | Loss: 0.6938\n",
      "Epoch: 0001/0003 | Batch 34500/114669 | Loss: 0.5806\n",
      "Epoch: 0001/0003 | Batch 34750/114669 | Loss: 0.5036\n",
      "Epoch: 0001/0003 | Batch 35000/114669 | Loss: 0.5648\n",
      "Epoch: 0001/0003 | Batch 35250/114669 | Loss: 0.3774\n",
      "Epoch: 0001/0003 | Batch 35500/114669 | Loss: 0.6888\n",
      "Epoch: 0001/0003 | Batch 35750/114669 | Loss: 0.6634\n",
      "Epoch: 0001/0003 | Batch 36000/114669 | Loss: 0.5722\n",
      "Epoch: 0001/0003 | Batch 36250/114669 | Loss: 0.6664\n",
      "Epoch: 0001/0003 | Batch 36500/114669 | Loss: 0.5872\n",
      "Epoch: 0001/0003 | Batch 36750/114669 | Loss: 0.5900\n",
      "Epoch: 0001/0003 | Batch 37000/114669 | Loss: 0.6635\n",
      "Epoch: 0001/0003 | Batch 37250/114669 | Loss: 0.8621\n",
      "Epoch: 0001/0003 | Batch 37500/114669 | Loss: 0.6750\n",
      "Epoch: 0001/0003 | Batch 37750/114669 | Loss: 0.5935\n",
      "Epoch: 0001/0003 | Batch 38000/114669 | Loss: 0.7500\n",
      "Epoch: 0001/0003 | Batch 38250/114669 | Loss: 0.7185\n",
      "Epoch: 0001/0003 | Batch 38500/114669 | Loss: 0.5921\n",
      "Epoch: 0001/0003 | Batch 38750/114669 | Loss: 0.7796\n",
      "Epoch: 0001/0003 | Batch 39000/114669 | Loss: 0.6700\n",
      "Epoch: 0001/0003 | Batch 39250/114669 | Loss: 0.5886\n",
      "Epoch: 0001/0003 | Batch 39500/114669 | Loss: 0.6577\n",
      "Epoch: 0001/0003 | Batch 39750/114669 | Loss: 0.5899\n",
      "Epoch: 0001/0003 | Batch 40000/114669 | Loss: 0.6609\n",
      "Epoch: 0001/0003 | Batch 40250/114669 | Loss: 0.6666\n",
      "Epoch: 0001/0003 | Batch 40500/114669 | Loss: 0.4809\n",
      "Epoch: 0001/0003 | Batch 40750/114669 | Loss: 0.6367\n",
      "Epoch: 0001/0003 | Batch 41000/114669 | Loss: 0.5234\n",
      "Epoch: 0001/0003 | Batch 41250/114669 | Loss: 0.6905\n",
      "Epoch: 0001/0003 | Batch 41500/114669 | Loss: 0.6783\n",
      "Epoch: 0001/0003 | Batch 41750/114669 | Loss: 0.6737\n",
      "Epoch: 0001/0003 | Batch 42000/114669 | Loss: 0.9144\n",
      "Epoch: 0001/0003 | Batch 42250/114669 | Loss: 0.8822\n",
      "Epoch: 0001/0003 | Batch 42500/114669 | Loss: 0.5606\n",
      "Epoch: 0001/0003 | Batch 42750/114669 | Loss: 0.6641\n",
      "Epoch: 0001/0003 | Batch 43000/114669 | Loss: 0.4335\n",
      "Epoch: 0001/0003 | Batch 43250/114669 | Loss: 0.6468\n",
      "Epoch: 0001/0003 | Batch 43500/114669 | Loss: 0.7889\n",
      "Epoch: 0001/0003 | Batch 43750/114669 | Loss: 0.4744\n",
      "Epoch: 0001/0003 | Batch 44000/114669 | Loss: 0.5750\n",
      "Epoch: 0001/0003 | Batch 44250/114669 | Loss: 0.3516\n",
      "Epoch: 0001/0003 | Batch 44500/114669 | Loss: 0.4850\n",
      "Epoch: 0001/0003 | Batch 44750/114669 | Loss: 0.6201\n",
      "Epoch: 0001/0003 | Batch 45000/114669 | Loss: 0.4836\n",
      "Epoch: 0001/0003 | Batch 45250/114669 | Loss: 1.0125\n",
      "Epoch: 0001/0003 | Batch 45500/114669 | Loss: 0.5482\n",
      "Epoch: 0001/0003 | Batch 45750/114669 | Loss: 0.4976\n",
      "Epoch: 0001/0003 | Batch 46000/114669 | Loss: 0.4293\n",
      "Epoch: 0001/0003 | Batch 46250/114669 | Loss: 0.5137\n",
      "Epoch: 0001/0003 | Batch 46500/114669 | Loss: 0.4041\n",
      "Epoch: 0001/0003 | Batch 46750/114669 | Loss: 0.6873\n",
      "Epoch: 0001/0003 | Batch 47000/114669 | Loss: 0.4396\n",
      "Epoch: 0001/0003 | Batch 47250/114669 | Loss: 0.7974\n",
      "Epoch: 0001/0003 | Batch 47500/114669 | Loss: 0.4697\n",
      "Epoch: 0001/0003 | Batch 47750/114669 | Loss: 0.6270\n",
      "Epoch: 0001/0003 | Batch 48000/114669 | Loss: 0.6999\n",
      "Epoch: 0001/0003 | Batch 48250/114669 | Loss: 0.6203\n",
      "Epoch: 0001/0003 | Batch 48500/114669 | Loss: 0.4644\n",
      "Epoch: 0001/0003 | Batch 48750/114669 | Loss: 0.5124\n",
      "Epoch: 0001/0003 | Batch 49000/114669 | Loss: 0.5333\n",
      "Epoch: 0001/0003 | Batch 49250/114669 | Loss: 0.6243\n",
      "Epoch: 0001/0003 | Batch 49500/114669 | Loss: 0.6878\n",
      "Epoch: 0001/0003 | Batch 49750/114669 | Loss: 0.6369\n",
      "Epoch: 0001/0003 | Batch 50000/114669 | Loss: 0.2211\n",
      "Epoch: 0001/0003 | Batch 50250/114669 | Loss: 0.7104\n",
      "Epoch: 0001/0003 | Batch 50500/114669 | Loss: 0.6478\n",
      "Epoch: 0001/0003 | Batch 50750/114669 | Loss: 0.3513\n",
      "Epoch: 0001/0003 | Batch 51000/114669 | Loss: 0.5984\n",
      "Epoch: 0001/0003 | Batch 51250/114669 | Loss: 0.3776\n",
      "Epoch: 0001/0003 | Batch 51500/114669 | Loss: 0.6577\n",
      "Epoch: 0001/0003 | Batch 51750/114669 | Loss: 0.5672\n",
      "Epoch: 0001/0003 | Batch 52000/114669 | Loss: 0.2268\n",
      "Epoch: 0001/0003 | Batch 52250/114669 | Loss: 0.2703\n",
      "Epoch: 0001/0003 | Batch 52500/114669 | Loss: 0.5690\n",
      "Epoch: 0001/0003 | Batch 52750/114669 | Loss: 0.2217\n",
      "Epoch: 0001/0003 | Batch 53000/114669 | Loss: 1.2652\n",
      "Epoch: 0001/0003 | Batch 53250/114669 | Loss: 0.4726\n",
      "Epoch: 0001/0003 | Batch 53500/114669 | Loss: 0.3142\n",
      "Epoch: 0001/0003 | Batch 53750/114669 | Loss: 0.5718\n",
      "Epoch: 0001/0003 | Batch 54000/114669 | Loss: 0.1922\n",
      "Epoch: 0001/0003 | Batch 54250/114669 | Loss: 0.3925\n",
      "Epoch: 0001/0003 | Batch 54500/114669 | Loss: 0.5262\n",
      "Epoch: 0001/0003 | Batch 54750/114669 | Loss: 0.5003\n",
      "Epoch: 0001/0003 | Batch 55000/114669 | Loss: 0.5537\n",
      "Epoch: 0001/0003 | Batch 55250/114669 | Loss: 0.4464\n",
      "Epoch: 0001/0003 | Batch 55500/114669 | Loss: 0.4071\n",
      "Epoch: 0001/0003 | Batch 55750/114669 | Loss: 0.3391\n",
      "Epoch: 0001/0003 | Batch 56000/114669 | Loss: 0.1495\n",
      "Epoch: 0001/0003 | Batch 56250/114669 | Loss: 0.2014\n",
      "Epoch: 0001/0003 | Batch 56500/114669 | Loss: 0.3490\n",
      "Epoch: 0001/0003 | Batch 56750/114669 | Loss: 0.1409\n",
      "Epoch: 0001/0003 | Batch 57000/114669 | Loss: 0.3453\n",
      "Epoch: 0001/0003 | Batch 57250/114669 | Loss: 0.5726\n",
      "Epoch: 0001/0003 | Batch 57500/114669 | Loss: 0.2032\n",
      "Epoch: 0001/0003 | Batch 57750/114669 | Loss: 0.1587\n",
      "Epoch: 0001/0003 | Batch 58000/114669 | Loss: 0.3716\n",
      "Epoch: 0001/0003 | Batch 58250/114669 | Loss: 0.1124\n",
      "Epoch: 0001/0003 | Batch 58500/114669 | Loss: 0.1820\n",
      "Epoch: 0001/0003 | Batch 58750/114669 | Loss: 0.2764\n",
      "Epoch: 0001/0003 | Batch 59000/114669 | Loss: 0.3210\n",
      "Epoch: 0001/0003 | Batch 59250/114669 | Loss: 0.9509\n",
      "Epoch: 0001/0003 | Batch 59500/114669 | Loss: 0.2000\n",
      "Epoch: 0001/0003 | Batch 59750/114669 | Loss: 0.5390\n",
      "Epoch: 0001/0003 | Batch 60000/114669 | Loss: 0.0884\n",
      "Epoch: 0001/0003 | Batch 60250/114669 | Loss: 0.3798\n",
      "Epoch: 0001/0003 | Batch 60500/114669 | Loss: 0.2429\n",
      "Epoch: 0001/0003 | Batch 60750/114669 | Loss: 0.1715\n",
      "Epoch: 0001/0003 | Batch 61000/114669 | Loss: 0.4881\n",
      "Epoch: 0001/0003 | Batch 61250/114669 | Loss: 0.6347\n",
      "Epoch: 0001/0003 | Batch 61500/114669 | Loss: 0.3872\n",
      "Epoch: 0001/0003 | Batch 61750/114669 | Loss: 0.7723\n",
      "Epoch: 0001/0003 | Batch 62000/114669 | Loss: 0.6295\n",
      "Epoch: 0001/0003 | Batch 62250/114669 | Loss: 0.4326\n",
      "Epoch: 0001/0003 | Batch 62500/114669 | Loss: 0.1200\n",
      "Epoch: 0001/0003 | Batch 62750/114669 | Loss: 0.4434\n",
      "Epoch: 0001/0003 | Batch 63000/114669 | Loss: 0.6652\n",
      "Epoch: 0001/0003 | Batch 63250/114669 | Loss: 0.9700\n",
      "Epoch: 0001/0003 | Batch 63500/114669 | Loss: 0.1925\n",
      "Epoch: 0001/0003 | Batch 63750/114669 | Loss: 0.7649\n",
      "Epoch: 0001/0003 | Batch 64000/114669 | Loss: 0.1343\n",
      "Epoch: 0001/0003 | Batch 64250/114669 | Loss: 0.5479\n",
      "Epoch: 0001/0003 | Batch 64500/114669 | Loss: 0.5808\n",
      "Epoch: 0001/0003 | Batch 64750/114669 | Loss: 0.4426\n",
      "Epoch: 0001/0003 | Batch 65000/114669 | Loss: 0.2459\n",
      "Epoch: 0001/0003 | Batch 65250/114669 | Loss: 0.2025\n",
      "Epoch: 0001/0003 | Batch 65500/114669 | Loss: 0.1950\n",
      "Epoch: 0001/0003 | Batch 65750/114669 | Loss: 0.0847\n",
      "Epoch: 0001/0003 | Batch 66000/114669 | Loss: 0.3776\n",
      "Epoch: 0001/0003 | Batch 66250/114669 | Loss: 0.6706\n",
      "Epoch: 0001/0003 | Batch 66500/114669 | Loss: 0.0700\n",
      "Epoch: 0001/0003 | Batch 66750/114669 | Loss: 0.3007\n",
      "Epoch: 0001/0003 | Batch 67000/114669 | Loss: 0.4250\n",
      "Epoch: 0001/0003 | Batch 67250/114669 | Loss: 0.5412\n",
      "Epoch: 0001/0003 | Batch 67500/114669 | Loss: 0.4416\n",
      "Epoch: 0001/0003 | Batch 67750/114669 | Loss: 0.3482\n",
      "Epoch: 0001/0003 | Batch 68000/114669 | Loss: 0.1646\n",
      "Epoch: 0001/0003 | Batch 68250/114669 | Loss: 0.3359\n",
      "Epoch: 0001/0003 | Batch 68500/114669 | Loss: 0.2818\n",
      "Epoch: 0001/0003 | Batch 68750/114669 | Loss: 0.3893\n",
      "Epoch: 0001/0003 | Batch 69000/114669 | Loss: 0.0993\n",
      "Epoch: 0001/0003 | Batch 69250/114669 | Loss: 0.5645\n",
      "Epoch: 0001/0003 | Batch 69500/114669 | Loss: 0.2475\n",
      "Epoch: 0001/0003 | Batch 69750/114669 | Loss: 0.2575\n",
      "Epoch: 0001/0003 | Batch 70000/114669 | Loss: 0.5982\n",
      "Epoch: 0001/0003 | Batch 70250/114669 | Loss: 0.1265\n",
      "Epoch: 0001/0003 | Batch 70500/114669 | Loss: 0.6749\n",
      "Epoch: 0001/0003 | Batch 70750/114669 | Loss: 0.6882\n",
      "Epoch: 0001/0003 | Batch 71000/114669 | Loss: 0.6188\n",
      "Epoch: 0001/0003 | Batch 71250/114669 | Loss: 0.1037\n",
      "Epoch: 0001/0003 | Batch 71500/114669 | Loss: 0.3385\n",
      "Epoch: 0001/0003 | Batch 71750/114669 | Loss: 0.5082\n",
      "Epoch: 0001/0003 | Batch 72000/114669 | Loss: 0.1577\n",
      "Epoch: 0001/0003 | Batch 72250/114669 | Loss: 0.2757\n",
      "Epoch: 0001/0003 | Batch 72500/114669 | Loss: 0.8953\n",
      "Epoch: 0001/0003 | Batch 72750/114669 | Loss: 0.3986\n",
      "Epoch: 0001/0003 | Batch 73000/114669 | Loss: 0.1690\n",
      "Epoch: 0001/0003 | Batch 73250/114669 | Loss: 0.1980\n",
      "Epoch: 0001/0003 | Batch 73500/114669 | Loss: 0.2242\n",
      "Epoch: 0001/0003 | Batch 73750/114669 | Loss: 0.8902\n",
      "Epoch: 0001/0003 | Batch 74000/114669 | Loss: 0.1189\n",
      "Epoch: 0001/0003 | Batch 74250/114669 | Loss: 0.3073\n",
      "Epoch: 0001/0003 | Batch 74500/114669 | Loss: 0.1210\n",
      "Epoch: 0001/0003 | Batch 74750/114669 | Loss: 0.4348\n",
      "Epoch: 0001/0003 | Batch 75000/114669 | Loss: 0.4221\n",
      "Epoch: 0001/0003 | Batch 75250/114669 | Loss: 0.3033\n",
      "Epoch: 0001/0003 | Batch 75500/114669 | Loss: 0.3597\n",
      "Epoch: 0001/0003 | Batch 75750/114669 | Loss: 0.4650\n",
      "Epoch: 0001/0003 | Batch 76000/114669 | Loss: 0.3852\n",
      "Epoch: 0001/0003 | Batch 76250/114669 | Loss: 0.5714\n",
      "Epoch: 0001/0003 | Batch 76500/114669 | Loss: 0.4855\n",
      "Epoch: 0001/0003 | Batch 76750/114669 | Loss: 0.6410\n",
      "Epoch: 0001/0003 | Batch 77000/114669 | Loss: 0.3436\n",
      "Epoch: 0001/0003 | Batch 77250/114669 | Loss: 0.0948\n",
      "Epoch: 0001/0003 | Batch 77500/114669 | Loss: 0.7826\n",
      "Epoch: 0001/0003 | Batch 77750/114669 | Loss: 0.1295\n",
      "Epoch: 0001/0003 | Batch 78000/114669 | Loss: 0.3942\n",
      "Epoch: 0001/0003 | Batch 78250/114669 | Loss: 0.3171\n",
      "Epoch: 0001/0003 | Batch 78500/114669 | Loss: 0.1790\n",
      "Epoch: 0001/0003 | Batch 78750/114669 | Loss: 0.3825\n",
      "Epoch: 0001/0003 | Batch 79000/114669 | Loss: 0.1251\n",
      "Epoch: 0001/0003 | Batch 79250/114669 | Loss: 0.2334\n",
      "Epoch: 0001/0003 | Batch 79500/114669 | Loss: 0.6674\n",
      "Epoch: 0001/0003 | Batch 79750/114669 | Loss: 0.4087\n",
      "Epoch: 0001/0003 | Batch 80000/114669 | Loss: 0.8198\n",
      "Epoch: 0001/0003 | Batch 80250/114669 | Loss: 0.4502\n",
      "Epoch: 0001/0003 | Batch 80500/114669 | Loss: 0.0913\n",
      "Epoch: 0001/0003 | Batch 80750/114669 | Loss: 0.1576\n",
      "Epoch: 0001/0003 | Batch 81000/114669 | Loss: 0.1258\n",
      "Epoch: 0001/0003 | Batch 81250/114669 | Loss: 0.6716\n",
      "Epoch: 0001/0003 | Batch 81500/114669 | Loss: 0.2441\n",
      "Epoch: 0001/0003 | Batch 81750/114669 | Loss: 0.2999\n",
      "Epoch: 0001/0003 | Batch 82000/114669 | Loss: 0.3840\n",
      "Epoch: 0001/0003 | Batch 82250/114669 | Loss: 0.1147\n",
      "Epoch: 0001/0003 | Batch 82500/114669 | Loss: 0.1542\n",
      "Epoch: 0001/0003 | Batch 82750/114669 | Loss: 0.2484\n",
      "Epoch: 0001/0003 | Batch 83000/114669 | Loss: 0.3550\n",
      "Epoch: 0001/0003 | Batch 83250/114669 | Loss: 0.1879\n",
      "Epoch: 0001/0003 | Batch 83500/114669 | Loss: 0.5655\n",
      "Epoch: 0001/0003 | Batch 83750/114669 | Loss: 0.4781\n",
      "Epoch: 0001/0003 | Batch 84000/114669 | Loss: 0.1398\n",
      "Epoch: 0001/0003 | Batch 84250/114669 | Loss: 0.3853\n",
      "Epoch: 0001/0003 | Batch 84500/114669 | Loss: 0.3591\n",
      "Epoch: 0001/0003 | Batch 84750/114669 | Loss: 0.4839\n",
      "Epoch: 0001/0003 | Batch 85000/114669 | Loss: 0.0733\n",
      "Epoch: 0001/0003 | Batch 85250/114669 | Loss: 0.6228\n",
      "Epoch: 0001/0003 | Batch 85500/114669 | Loss: 1.0817\n",
      "Epoch: 0001/0003 | Batch 85750/114669 | Loss: 0.2432\n",
      "Epoch: 0001/0003 | Batch 86000/114669 | Loss: 0.0622\n",
      "Epoch: 0001/0003 | Batch 86250/114669 | Loss: 0.4076\n",
      "Epoch: 0001/0003 | Batch 86500/114669 | Loss: 0.3542\n",
      "Epoch: 0001/0003 | Batch 86750/114669 | Loss: 0.4416\n",
      "Epoch: 0001/0003 | Batch 87000/114669 | Loss: 0.3786\n",
      "Epoch: 0001/0003 | Batch 87250/114669 | Loss: 0.2938\n",
      "Epoch: 0001/0003 | Batch 87500/114669 | Loss: 0.1489\n",
      "Epoch: 0001/0003 | Batch 87750/114669 | Loss: 0.3941\n",
      "Epoch: 0001/0003 | Batch 88000/114669 | Loss: 0.5682\n",
      "Epoch: 0001/0003 | Batch 88250/114669 | Loss: 0.3847\n",
      "Epoch: 0001/0003 | Batch 88500/114669 | Loss: 0.0368\n",
      "Epoch: 0001/0003 | Batch 88750/114669 | Loss: 0.0641\n",
      "Epoch: 0001/0003 | Batch 89000/114669 | Loss: 0.3224\n",
      "Epoch: 0001/0003 | Batch 89250/114669 | Loss: 0.1260\n",
      "Epoch: 0001/0003 | Batch 89500/114669 | Loss: 0.2316\n",
      "Epoch: 0001/0003 | Batch 89750/114669 | Loss: 0.1120\n",
      "Epoch: 0001/0003 | Batch 90000/114669 | Loss: 0.3007\n",
      "Epoch: 0001/0003 | Batch 90250/114669 | Loss: 0.2529\n",
      "Epoch: 0001/0003 | Batch 90500/114669 | Loss: 0.4442\n",
      "Epoch: 0001/0003 | Batch 90750/114669 | Loss: 0.4037\n",
      "Epoch: 0001/0003 | Batch 91000/114669 | Loss: 0.2585\n",
      "Epoch: 0001/0003 | Batch 91250/114669 | Loss: 0.1640\n",
      "Epoch: 0001/0003 | Batch 91500/114669 | Loss: 0.3339\n",
      "Epoch: 0001/0003 | Batch 91750/114669 | Loss: 0.1952\n",
      "Epoch: 0001/0003 | Batch 92000/114669 | Loss: 0.6489\n",
      "Epoch: 0001/0003 | Batch 92250/114669 | Loss: 0.7023\n",
      "Epoch: 0001/0003 | Batch 92500/114669 | Loss: 0.2075\n",
      "Epoch: 0001/0003 | Batch 92750/114669 | Loss: 0.1214\n",
      "Epoch: 0001/0003 | Batch 93000/114669 | Loss: 0.6548\n",
      "Epoch: 0001/0003 | Batch 93250/114669 | Loss: 0.0881\n",
      "Epoch: 0001/0003 | Batch 93500/114669 | Loss: 0.3968\n",
      "Epoch: 0001/0003 | Batch 93750/114669 | Loss: 0.2344\n",
      "Epoch: 0001/0003 | Batch 94000/114669 | Loss: 0.2640\n",
      "Epoch: 0001/0003 | Batch 94250/114669 | Loss: 0.1373\n",
      "Epoch: 0001/0003 | Batch 94500/114669 | Loss: 0.1140\n",
      "Epoch: 0001/0003 | Batch 94750/114669 | Loss: 0.2716\n",
      "Epoch: 0001/0003 | Batch 95000/114669 | Loss: 0.3464\n",
      "Epoch: 0001/0003 | Batch 95250/114669 | Loss: 0.4848\n",
      "Epoch: 0001/0003 | Batch 95500/114669 | Loss: 0.1782\n",
      "Epoch: 0001/0003 | Batch 95750/114669 | Loss: 0.3844\n",
      "Epoch: 0001/0003 | Batch 96000/114669 | Loss: 0.0775\n",
      "Epoch: 0001/0003 | Batch 96250/114669 | Loss: 0.1115\n",
      "Epoch: 0001/0003 | Batch 96500/114669 | Loss: 0.3852\n",
      "Epoch: 0001/0003 | Batch 96750/114669 | Loss: 0.7484\n",
      "Epoch: 0001/0003 | Batch 97000/114669 | Loss: 0.0883\n",
      "Epoch: 0001/0003 | Batch 97250/114669 | Loss: 0.3651\n",
      "Epoch: 0001/0003 | Batch 97500/114669 | Loss: 0.2804\n",
      "Epoch: 0001/0003 | Batch 97750/114669 | Loss: 0.7562\n",
      "Epoch: 0001/0003 | Batch 98000/114669 | Loss: 0.1058\n",
      "Epoch: 0001/0003 | Batch 98250/114669 | Loss: 0.0806\n",
      "Epoch: 0001/0003 | Batch 98500/114669 | Loss: 0.4776\n",
      "Epoch: 0001/0003 | Batch 98750/114669 | Loss: 0.5126\n",
      "Epoch: 0001/0003 | Batch 99000/114669 | Loss: 0.0771\n",
      "Epoch: 0001/0003 | Batch 99250/114669 | Loss: 0.4243\n",
      "Epoch: 0001/0003 | Batch 99500/114669 | Loss: 0.3934\n",
      "Epoch: 0001/0003 | Batch 99750/114669 | Loss: 0.1187\n",
      "Epoch: 0001/0003 | Batch 100000/114669 | Loss: 0.3090\n",
      "Epoch: 0001/0003 | Batch 100250/114669 | Loss: 0.3181\n",
      "Epoch: 0001/0003 | Batch 100500/114669 | Loss: 0.1623\n",
      "Epoch: 0001/0003 | Batch 100750/114669 | Loss: 0.1561\n",
      "Epoch: 0001/0003 | Batch 101000/114669 | Loss: 0.1507\n",
      "Epoch: 0001/0003 | Batch 101250/114669 | Loss: 0.2348\n",
      "Epoch: 0001/0003 | Batch 101500/114669 | Loss: 0.6581\n",
      "Epoch: 0001/0003 | Batch 101750/114669 | Loss: 0.1231\n",
      "Epoch: 0001/0003 | Batch 102000/114669 | Loss: 0.7709\n",
      "Epoch: 0001/0003 | Batch 102250/114669 | Loss: 0.2694\n",
      "Epoch: 0001/0003 | Batch 102500/114669 | Loss: 0.5097\n",
      "Epoch: 0001/0003 | Batch 102750/114669 | Loss: 0.4912\n",
      "Epoch: 0001/0003 | Batch 103000/114669 | Loss: 0.4175\n",
      "Epoch: 0001/0003 | Batch 103250/114669 | Loss: 0.9525\n",
      "Epoch: 0001/0003 | Batch 103500/114669 | Loss: 0.1976\n",
      "Epoch: 0001/0003 | Batch 103750/114669 | Loss: 0.0953\n",
      "Epoch: 0001/0003 | Batch 104000/114669 | Loss: 0.3748\n",
      "Epoch: 0001/0003 | Batch 104250/114669 | Loss: 0.6007\n",
      "Epoch: 0001/0003 | Batch 104500/114669 | Loss: 0.2993\n",
      "Epoch: 0001/0003 | Batch 104750/114669 | Loss: 0.1007\n",
      "Epoch: 0001/0003 | Batch 105000/114669 | Loss: 0.7966\n",
      "Epoch: 0001/0003 | Batch 105250/114669 | Loss: 0.2938\n",
      "Epoch: 0001/0003 | Batch 105500/114669 | Loss: 0.6217\n",
      "Epoch: 0001/0003 | Batch 105750/114669 | Loss: 0.4087\n",
      "Epoch: 0001/0003 | Batch 106000/114669 | Loss: 0.2136\n",
      "Epoch: 0001/0003 | Batch 106250/114669 | Loss: 0.0946\n",
      "Epoch: 0001/0003 | Batch 106500/114669 | Loss: 0.3771\n",
      "Epoch: 0001/0003 | Batch 106750/114669 | Loss: 0.2806\n",
      "Epoch: 0001/0003 | Batch 107000/114669 | Loss: 0.0877\n",
      "Epoch: 0001/0003 | Batch 107250/114669 | Loss: 0.3569\n",
      "Epoch: 0001/0003 | Batch 107500/114669 | Loss: 0.1926\n",
      "Epoch: 0001/0003 | Batch 107750/114669 | Loss: 0.0998\n",
      "Epoch: 0001/0003 | Batch 108000/114669 | Loss: 0.0812\n",
      "Epoch: 0001/0003 | Batch 108250/114669 | Loss: 0.1132\n",
      "Epoch: 0001/0003 | Batch 108500/114669 | Loss: 0.1105\n",
      "Epoch: 0001/0003 | Batch 108750/114669 | Loss: 0.5231\n",
      "Epoch: 0001/0003 | Batch 109000/114669 | Loss: 0.5193\n",
      "Epoch: 0001/0003 | Batch 109250/114669 | Loss: 0.3024\n",
      "Epoch: 0001/0003 | Batch 109500/114669 | Loss: 0.0983\n",
      "Epoch: 0001/0003 | Batch 109750/114669 | Loss: 0.0963\n",
      "Epoch: 0001/0003 | Batch 110000/114669 | Loss: 0.0766\n",
      "Epoch: 0001/0003 | Batch 110250/114669 | Loss: 0.2436\n",
      "Epoch: 0001/0003 | Batch 110500/114669 | Loss: 0.3967\n",
      "Epoch: 0001/0003 | Batch 110750/114669 | Loss: 0.0901\n",
      "Epoch: 0001/0003 | Batch 111000/114669 | Loss: 0.3614\n",
      "Epoch: 0001/0003 | Batch 111250/114669 | Loss: 0.1455\n",
      "Epoch: 0001/0003 | Batch 111500/114669 | Loss: 0.4365\n",
      "Epoch: 0001/0003 | Batch 111750/114669 | Loss: 0.4315\n",
      "Epoch: 0001/0003 | Batch 112000/114669 | Loss: 0.3680\n",
      "Epoch: 0001/0003 | Batch 112250/114669 | Loss: 0.0577\n",
      "Epoch: 0001/0003 | Batch 112500/114669 | Loss: 0.2714\n",
      "Epoch: 0001/0003 | Batch 112750/114669 | Loss: 0.2715\n",
      "Epoch: 0001/0003 | Batch 113000/114669 | Loss: 0.3908\n",
      "Epoch: 0001/0003 | Batch 113250/114669 | Loss: 0.1242\n",
      "Epoch: 0001/0003 | Batch 113500/114669 | Loss: 0.2274\n",
      "Epoch: 0001/0003 | Batch 113750/114669 | Loss: 0.6203\n",
      "Epoch: 0001/0003 | Batch 114000/114669 | Loss: 0.2928\n",
      "Epoch: 0001/0003 | Batch 114250/114669 | Loss: 0.6865\n",
      "Epoch: 0001/0003 | Batch 114500/114669 | Loss: 0.2497\n",
      "Training accuracy: 31.91%\n",
      "Valid accuracy: 31.83%\n",
      "Time elapsed: 94.40 min\n",
      "Epoch: 0002/0003 | Batch 0000/114669 | Loss: 0.1011\n",
      "Epoch: 0002/0003 | Batch 0250/114669 | Loss: 0.0693\n",
      "Epoch: 0002/0003 | Batch 0500/114669 | Loss: 0.6596\n",
      "Epoch: 0002/0003 | Batch 0750/114669 | Loss: 0.5272\n",
      "Epoch: 0002/0003 | Batch 1000/114669 | Loss: 0.4987\n",
      "Epoch: 0002/0003 | Batch 1250/114669 | Loss: 0.2415\n",
      "Epoch: 0002/0003 | Batch 1500/114669 | Loss: 0.4121\n",
      "Epoch: 0002/0003 | Batch 1750/114669 | Loss: 0.8202\n",
      "Epoch: 0002/0003 | Batch 2000/114669 | Loss: 0.7811\n",
      "Epoch: 0002/0003 | Batch 2250/114669 | Loss: 0.3787\n",
      "Epoch: 0002/0003 | Batch 2500/114669 | Loss: 0.1084\n",
      "Epoch: 0002/0003 | Batch 2750/114669 | Loss: 0.0679\n",
      "Epoch: 0002/0003 | Batch 3000/114669 | Loss: 0.0776\n",
      "Epoch: 0002/0003 | Batch 3250/114669 | Loss: 0.2990\n",
      "Epoch: 0002/0003 | Batch 3500/114669 | Loss: 0.1972\n",
      "Epoch: 0002/0003 | Batch 3750/114669 | Loss: 0.7797\n",
      "Epoch: 0002/0003 | Batch 4000/114669 | Loss: 0.0960\n",
      "Epoch: 0002/0003 | Batch 4250/114669 | Loss: 0.0865\n",
      "Epoch: 0002/0003 | Batch 4500/114669 | Loss: 0.4400\n",
      "Epoch: 0002/0003 | Batch 4750/114669 | Loss: 0.1130\n",
      "Epoch: 0002/0003 | Batch 5000/114669 | Loss: 0.5947\n",
      "Epoch: 0002/0003 | Batch 5250/114669 | Loss: 0.2030\n",
      "Epoch: 0002/0003 | Batch 5500/114669 | Loss: 0.2977\n",
      "Epoch: 0002/0003 | Batch 5750/114669 | Loss: 0.3482\n",
      "Epoch: 0002/0003 | Batch 6000/114669 | Loss: 0.3512\n",
      "Epoch: 0002/0003 | Batch 6250/114669 | Loss: 0.4574\n",
      "Epoch: 0002/0003 | Batch 6500/114669 | Loss: 0.5947\n",
      "Epoch: 0002/0003 | Batch 6750/114669 | Loss: 0.1796\n",
      "Epoch: 0002/0003 | Batch 7000/114669 | Loss: 0.4315\n",
      "Epoch: 0002/0003 | Batch 7250/114669 | Loss: 0.0353\n",
      "Epoch: 0002/0003 | Batch 7500/114669 | Loss: 0.3945\n",
      "Epoch: 0002/0003 | Batch 7750/114669 | Loss: 0.6568\n",
      "Epoch: 0002/0003 | Batch 8000/114669 | Loss: 0.1427\n",
      "Epoch: 0002/0003 | Batch 8250/114669 | Loss: 0.7152\n",
      "Epoch: 0002/0003 | Batch 8500/114669 | Loss: 0.3024\n",
      "Epoch: 0002/0003 | Batch 8750/114669 | Loss: 0.2960\n",
      "Epoch: 0002/0003 | Batch 9000/114669 | Loss: 0.8225\n",
      "Epoch: 0002/0003 | Batch 9250/114669 | Loss: 0.2756\n",
      "Epoch: 0002/0003 | Batch 9500/114669 | Loss: 0.7060\n",
      "Epoch: 0002/0003 | Batch 9750/114669 | Loss: 0.3343\n",
      "Epoch: 0002/0003 | Batch 10000/114669 | Loss: 0.1813\n",
      "Epoch: 0002/0003 | Batch 10250/114669 | Loss: 0.4258\n",
      "Epoch: 0002/0003 | Batch 10500/114669 | Loss: 0.5572\n",
      "Epoch: 0002/0003 | Batch 10750/114669 | Loss: 0.1206\n",
      "Epoch: 0002/0003 | Batch 11000/114669 | Loss: 0.3125\n",
      "Epoch: 0002/0003 | Batch 11250/114669 | Loss: 0.4514\n",
      "Epoch: 0002/0003 | Batch 11500/114669 | Loss: 0.1255\n",
      "Epoch: 0002/0003 | Batch 11750/114669 | Loss: 0.3127\n",
      "Epoch: 0002/0003 | Batch 12000/114669 | Loss: 0.5514\n",
      "Epoch: 0002/0003 | Batch 12250/114669 | Loss: 0.2815\n",
      "Epoch: 0002/0003 | Batch 12500/114669 | Loss: 0.8170\n",
      "Epoch: 0002/0003 | Batch 12750/114669 | Loss: 0.1649\n",
      "Epoch: 0002/0003 | Batch 13000/114669 | Loss: 0.2355\n",
      "Epoch: 0002/0003 | Batch 13250/114669 | Loss: 0.1339\n",
      "Epoch: 0002/0003 | Batch 13500/114669 | Loss: 0.5827\n",
      "Epoch: 0002/0003 | Batch 13750/114669 | Loss: 0.5159\n",
      "Epoch: 0002/0003 | Batch 14000/114669 | Loss: 0.1245\n",
      "Epoch: 0002/0003 | Batch 14250/114669 | Loss: 0.1705\n",
      "Epoch: 0002/0003 | Batch 14500/114669 | Loss: 0.1294\n",
      "Epoch: 0002/0003 | Batch 14750/114669 | Loss: 0.0623\n",
      "Epoch: 0002/0003 | Batch 15000/114669 | Loss: 0.5057\n",
      "Epoch: 0002/0003 | Batch 15250/114669 | Loss: 0.1870\n",
      "Epoch: 0002/0003 | Batch 15500/114669 | Loss: 0.0771\n",
      "Epoch: 0002/0003 | Batch 15750/114669 | Loss: 0.0491\n",
      "Epoch: 0002/0003 | Batch 16000/114669 | Loss: 0.4284\n",
      "Epoch: 0002/0003 | Batch 16250/114669 | Loss: 0.1398\n",
      "Epoch: 0002/0003 | Batch 16500/114669 | Loss: 0.1210\n",
      "Epoch: 0002/0003 | Batch 16750/114669 | Loss: 0.3448\n",
      "Epoch: 0002/0003 | Batch 17000/114669 | Loss: 0.1715\n",
      "Epoch: 0002/0003 | Batch 17250/114669 | Loss: 0.3476\n",
      "Epoch: 0002/0003 | Batch 17500/114669 | Loss: 0.1277\n",
      "Epoch: 0002/0003 | Batch 17750/114669 | Loss: 0.4420\n",
      "Epoch: 0002/0003 | Batch 18000/114669 | Loss: 0.1294\n",
      "Epoch: 0002/0003 | Batch 18250/114669 | Loss: 0.2429\n",
      "Epoch: 0002/0003 | Batch 18500/114669 | Loss: 0.5668\n",
      "Epoch: 0002/0003 | Batch 18750/114669 | Loss: 0.3763\n",
      "Epoch: 0002/0003 | Batch 19000/114669 | Loss: 0.0796\n",
      "Epoch: 0002/0003 | Batch 19250/114669 | Loss: 0.4114\n",
      "Epoch: 0002/0003 | Batch 19500/114669 | Loss: 0.0958\n",
      "Epoch: 0002/0003 | Batch 19750/114669 | Loss: 0.0912\n",
      "Epoch: 0002/0003 | Batch 20000/114669 | Loss: 0.4011\n",
      "Epoch: 0002/0003 | Batch 20250/114669 | Loss: 0.1096\n",
      "Epoch: 0002/0003 | Batch 20500/114669 | Loss: 0.4108\n",
      "Epoch: 0002/0003 | Batch 20750/114669 | Loss: 0.0808\n",
      "Epoch: 0002/0003 | Batch 21000/114669 | Loss: 0.7330\n",
      "Epoch: 0002/0003 | Batch 21250/114669 | Loss: 0.0928\n",
      "Epoch: 0002/0003 | Batch 21500/114669 | Loss: 0.2429\n",
      "Epoch: 0002/0003 | Batch 21750/114669 | Loss: 0.2630\n",
      "Epoch: 0002/0003 | Batch 22000/114669 | Loss: 0.0862\n",
      "Epoch: 0002/0003 | Batch 22250/114669 | Loss: 0.1038\n",
      "Epoch: 0002/0003 | Batch 22500/114669 | Loss: 0.6278\n",
      "Epoch: 0002/0003 | Batch 22750/114669 | Loss: 0.4950\n",
      "Epoch: 0002/0003 | Batch 23000/114669 | Loss: 0.0604\n",
      "Epoch: 0002/0003 | Batch 23250/114669 | Loss: 0.2535\n",
      "Epoch: 0002/0003 | Batch 23500/114669 | Loss: 0.0625\n",
      "Epoch: 0002/0003 | Batch 23750/114669 | Loss: 0.8061\n",
      "Epoch: 0002/0003 | Batch 24000/114669 | Loss: 0.5330\n",
      "Epoch: 0002/0003 | Batch 24250/114669 | Loss: 0.2489\n",
      "Epoch: 0002/0003 | Batch 24500/114669 | Loss: 0.7928\n",
      "Epoch: 0002/0003 | Batch 24750/114669 | Loss: 0.0804\n",
      "Epoch: 0002/0003 | Batch 25000/114669 | Loss: 0.3291\n",
      "Epoch: 0002/0003 | Batch 25250/114669 | Loss: 0.0622\n",
      "Epoch: 0002/0003 | Batch 25500/114669 | Loss: 0.3948\n",
      "Epoch: 0002/0003 | Batch 25750/114669 | Loss: 0.4230\n",
      "Epoch: 0002/0003 | Batch 26000/114669 | Loss: 0.1020\n",
      "Epoch: 0002/0003 | Batch 26250/114669 | Loss: 0.0611\n",
      "Epoch: 0002/0003 | Batch 26500/114669 | Loss: 0.3246\n",
      "Epoch: 0002/0003 | Batch 26750/114669 | Loss: 0.0821\n",
      "Epoch: 0002/0003 | Batch 27000/114669 | Loss: 0.3242\n",
      "Epoch: 0002/0003 | Batch 27250/114669 | Loss: 0.0640\n",
      "Epoch: 0002/0003 | Batch 27500/114669 | Loss: 0.4173\n",
      "Epoch: 0002/0003 | Batch 27750/114669 | Loss: 0.5221\n",
      "Epoch: 0002/0003 | Batch 28000/114669 | Loss: 0.2007\n",
      "Epoch: 0002/0003 | Batch 28250/114669 | Loss: 0.6294\n",
      "Epoch: 0002/0003 | Batch 28500/114669 | Loss: 0.0758\n",
      "Epoch: 0002/0003 | Batch 28750/114669 | Loss: 0.0703\n",
      "Epoch: 0002/0003 | Batch 29000/114669 | Loss: 0.6737\n",
      "Epoch: 0002/0003 | Batch 29250/114669 | Loss: 0.3716\n",
      "Epoch: 0002/0003 | Batch 29500/114669 | Loss: 0.0832\n",
      "Epoch: 0002/0003 | Batch 29750/114669 | Loss: 0.0879\n",
      "Epoch: 0002/0003 | Batch 30000/114669 | Loss: 0.4199\n",
      "Epoch: 0002/0003 | Batch 30250/114669 | Loss: 0.4458\n",
      "Epoch: 0002/0003 | Batch 30500/114669 | Loss: 0.2525\n",
      "Epoch: 0002/0003 | Batch 30750/114669 | Loss: 0.0827\n",
      "Epoch: 0002/0003 | Batch 31000/114669 | Loss: 0.3876\n",
      "Epoch: 0002/0003 | Batch 31250/114669 | Loss: 0.1081\n",
      "Epoch: 0002/0003 | Batch 31500/114669 | Loss: 0.0553\n",
      "Epoch: 0002/0003 | Batch 31750/114669 | Loss: 0.2836\n",
      "Epoch: 0002/0003 | Batch 32000/114669 | Loss: 0.4345\n",
      "Epoch: 0002/0003 | Batch 32250/114669 | Loss: 0.3490\n",
      "Epoch: 0002/0003 | Batch 32500/114669 | Loss: 0.4176\n",
      "Epoch: 0002/0003 | Batch 32750/114669 | Loss: 0.6271\n",
      "Epoch: 0002/0003 | Batch 33000/114669 | Loss: 0.7616\n",
      "Epoch: 0002/0003 | Batch 33250/114669 | Loss: 0.1000\n",
      "Epoch: 0002/0003 | Batch 33500/114669 | Loss: 0.1168\n",
      "Epoch: 0002/0003 | Batch 33750/114669 | Loss: 0.0872\n",
      "Epoch: 0002/0003 | Batch 34000/114669 | Loss: 0.0932\n",
      "Epoch: 0002/0003 | Batch 34250/114669 | Loss: 0.3867\n",
      "Epoch: 0002/0003 | Batch 34500/114669 | Loss: 0.2461\n",
      "Epoch: 0002/0003 | Batch 34750/114669 | Loss: 0.2234\n",
      "Epoch: 0002/0003 | Batch 35000/114669 | Loss: 0.5081\n",
      "Epoch: 0002/0003 | Batch 35250/114669 | Loss: 0.2493\n",
      "Epoch: 0002/0003 | Batch 35500/114669 | Loss: 0.3300\n",
      "Epoch: 0002/0003 | Batch 35750/114669 | Loss: 0.0800\n",
      "Epoch: 0002/0003 | Batch 36000/114669 | Loss: 0.0592\n",
      "Epoch: 0002/0003 | Batch 36250/114669 | Loss: 0.1188\n",
      "Epoch: 0002/0003 | Batch 36500/114669 | Loss: 0.2120\n",
      "Epoch: 0002/0003 | Batch 36750/114669 | Loss: 0.3357\n",
      "Epoch: 0002/0003 | Batch 37000/114669 | Loss: 0.5312\n",
      "Epoch: 0002/0003 | Batch 37250/114669 | Loss: 0.3965\n",
      "Epoch: 0002/0003 | Batch 37500/114669 | Loss: 0.1319\n",
      "Epoch: 0002/0003 | Batch 37750/114669 | Loss: 0.1095\n",
      "Epoch: 0002/0003 | Batch 38000/114669 | Loss: 0.3069\n",
      "Epoch: 0002/0003 | Batch 38250/114669 | Loss: 0.2587\n",
      "Epoch: 0002/0003 | Batch 38500/114669 | Loss: 0.1789\n",
      "Epoch: 0002/0003 | Batch 38750/114669 | Loss: 0.1551\n",
      "Epoch: 0002/0003 | Batch 39000/114669 | Loss: 0.4342\n",
      "Epoch: 0002/0003 | Batch 39250/114669 | Loss: 0.5529\n",
      "Epoch: 0002/0003 | Batch 39500/114669 | Loss: 0.4694\n",
      "Epoch: 0002/0003 | Batch 39750/114669 | Loss: 0.1801\n",
      "Epoch: 0002/0003 | Batch 40000/114669 | Loss: 0.0838\n",
      "Epoch: 0002/0003 | Batch 40250/114669 | Loss: 0.3659\n",
      "Epoch: 0002/0003 | Batch 40500/114669 | Loss: 0.6127\n",
      "Epoch: 0002/0003 | Batch 40750/114669 | Loss: 0.3724\n",
      "Epoch: 0002/0003 | Batch 41000/114669 | Loss: 0.5301\n",
      "Epoch: 0002/0003 | Batch 41250/114669 | Loss: 0.5623\n",
      "Epoch: 0002/0003 | Batch 41500/114669 | Loss: 0.3772\n",
      "Epoch: 0002/0003 | Batch 41750/114669 | Loss: 0.3574\n",
      "Epoch: 0002/0003 | Batch 42000/114669 | Loss: 0.3996\n",
      "Epoch: 0002/0003 | Batch 42250/114669 | Loss: 0.4266\n",
      "Epoch: 0002/0003 | Batch 42500/114669 | Loss: 0.2396\n",
      "Epoch: 0002/0003 | Batch 42750/114669 | Loss: 0.1710\n",
      "Epoch: 0002/0003 | Batch 43000/114669 | Loss: 0.9231\n",
      "Epoch: 0002/0003 | Batch 43250/114669 | Loss: 0.2178\n",
      "Epoch: 0002/0003 | Batch 43500/114669 | Loss: 0.1393\n",
      "Epoch: 0002/0003 | Batch 43750/114669 | Loss: 0.0507\n",
      "Epoch: 0002/0003 | Batch 44000/114669 | Loss: 0.3711\n",
      "Epoch: 0002/0003 | Batch 44250/114669 | Loss: 0.0559\n",
      "Epoch: 0002/0003 | Batch 44500/114669 | Loss: 1.4072\n",
      "Epoch: 0002/0003 | Batch 44750/114669 | Loss: 0.0813\n",
      "Epoch: 0002/0003 | Batch 45000/114669 | Loss: 0.2489\n",
      "Epoch: 0002/0003 | Batch 45250/114669 | Loss: 0.5189\n",
      "Epoch: 0002/0003 | Batch 45500/114669 | Loss: 0.6227\n",
      "Epoch: 0002/0003 | Batch 45750/114669 | Loss: 0.2083\n",
      "Epoch: 0002/0003 | Batch 46000/114669 | Loss: 0.0709\n",
      "Epoch: 0002/0003 | Batch 46250/114669 | Loss: 0.1753\n",
      "Epoch: 0002/0003 | Batch 46500/114669 | Loss: 0.3193\n",
      "Epoch: 0002/0003 | Batch 46750/114669 | Loss: 0.3493\n",
      "Epoch: 0002/0003 | Batch 47000/114669 | Loss: 0.1653\n",
      "Epoch: 0002/0003 | Batch 47250/114669 | Loss: 0.5692\n",
      "Epoch: 0002/0003 | Batch 47500/114669 | Loss: 0.0397\n",
      "Epoch: 0002/0003 | Batch 47750/114669 | Loss: 0.3336\n",
      "Epoch: 0002/0003 | Batch 48000/114669 | Loss: 0.0704\n",
      "Epoch: 0002/0003 | Batch 48250/114669 | Loss: 0.2795\n",
      "Epoch: 0002/0003 | Batch 48500/114669 | Loss: 0.3619\n",
      "Epoch: 0002/0003 | Batch 48750/114669 | Loss: 0.3505\n",
      "Epoch: 0002/0003 | Batch 49000/114669 | Loss: 0.0771\n",
      "Epoch: 0002/0003 | Batch 49250/114669 | Loss: 0.4040\n",
      "Epoch: 0002/0003 | Batch 49500/114669 | Loss: 0.0659\n",
      "Epoch: 0002/0003 | Batch 49750/114669 | Loss: 0.2958\n",
      "Epoch: 0002/0003 | Batch 50000/114669 | Loss: 0.4606\n",
      "Epoch: 0002/0003 | Batch 50250/114669 | Loss: 0.4296\n",
      "Epoch: 0002/0003 | Batch 50500/114669 | Loss: 0.4179\n",
      "Epoch: 0002/0003 | Batch 50750/114669 | Loss: 0.2822\n",
      "Epoch: 0002/0003 | Batch 51000/114669 | Loss: 0.1303\n",
      "Epoch: 0002/0003 | Batch 51250/114669 | Loss: 0.0718\n",
      "Epoch: 0002/0003 | Batch 51500/114669 | Loss: 0.4106\n",
      "Epoch: 0002/0003 | Batch 51750/114669 | Loss: 0.4330\n",
      "Epoch: 0002/0003 | Batch 52000/114669 | Loss: 0.1865\n",
      "Epoch: 0002/0003 | Batch 52250/114669 | Loss: 0.2621\n",
      "Epoch: 0002/0003 | Batch 52500/114669 | Loss: 0.3162\n",
      "Epoch: 0002/0003 | Batch 52750/114669 | Loss: 0.3160\n",
      "Epoch: 0002/0003 | Batch 53000/114669 | Loss: 0.3240\n",
      "Epoch: 0002/0003 | Batch 53250/114669 | Loss: 0.5745\n",
      "Epoch: 0002/0003 | Batch 53500/114669 | Loss: 0.5868\n",
      "Epoch: 0002/0003 | Batch 53750/114669 | Loss: 0.0806\n",
      "Epoch: 0002/0003 | Batch 54000/114669 | Loss: 0.2796\n",
      "Epoch: 0002/0003 | Batch 54250/114669 | Loss: 0.2751\n",
      "Epoch: 0002/0003 | Batch 54500/114669 | Loss: 0.4745\n",
      "Epoch: 0002/0003 | Batch 54750/114669 | Loss: 0.3321\n",
      "Epoch: 0002/0003 | Batch 55000/114669 | Loss: 0.3539\n",
      "Epoch: 0002/0003 | Batch 55250/114669 | Loss: 0.4620\n",
      "Epoch: 0002/0003 | Batch 55500/114669 | Loss: 0.2594\n",
      "Epoch: 0002/0003 | Batch 55750/114669 | Loss: 0.3178\n",
      "Epoch: 0002/0003 | Batch 56000/114669 | Loss: 0.0832\n",
      "Epoch: 0002/0003 | Batch 56250/114669 | Loss: 0.2523\n",
      "Epoch: 0002/0003 | Batch 56500/114669 | Loss: 0.3818\n",
      "Epoch: 0002/0003 | Batch 56750/114669 | Loss: 0.0715\n",
      "Epoch: 0002/0003 | Batch 57000/114669 | Loss: 0.1561\n",
      "Epoch: 0002/0003 | Batch 57250/114669 | Loss: 0.2954\n",
      "Epoch: 0002/0003 | Batch 57500/114669 | Loss: 0.6967\n",
      "Epoch: 0002/0003 | Batch 57750/114669 | Loss: 0.6809\n",
      "Epoch: 0002/0003 | Batch 58000/114669 | Loss: 0.1474\n",
      "Epoch: 0002/0003 | Batch 58250/114669 | Loss: 0.1809\n",
      "Epoch: 0002/0003 | Batch 58500/114669 | Loss: 0.2619\n",
      "Epoch: 0002/0003 | Batch 58750/114669 | Loss: 0.4684\n",
      "Epoch: 0002/0003 | Batch 59000/114669 | Loss: 0.5746\n",
      "Epoch: 0002/0003 | Batch 59250/114669 | Loss: 0.2057\n",
      "Epoch: 0002/0003 | Batch 59500/114669 | Loss: 0.1145\n",
      "Epoch: 0002/0003 | Batch 59750/114669 | Loss: 0.3209\n",
      "Epoch: 0002/0003 | Batch 60000/114669 | Loss: 0.3635\n",
      "Epoch: 0002/0003 | Batch 60250/114669 | Loss: 0.2931\n",
      "Epoch: 0002/0003 | Batch 60500/114669 | Loss: 0.3966\n",
      "Epoch: 0002/0003 | Batch 60750/114669 | Loss: 0.0408\n",
      "Epoch: 0002/0003 | Batch 61000/114669 | Loss: 0.0892\n",
      "Epoch: 0002/0003 | Batch 61250/114669 | Loss: 0.0745\n",
      "Epoch: 0002/0003 | Batch 61500/114669 | Loss: 0.3193\n",
      "Epoch: 0002/0003 | Batch 61750/114669 | Loss: 0.1249\n",
      "Epoch: 0002/0003 | Batch 62000/114669 | Loss: 0.1864\n",
      "Epoch: 0002/0003 | Batch 62250/114669 | Loss: 0.2134\n",
      "Epoch: 0002/0003 | Batch 62500/114669 | Loss: 0.4502\n",
      "Epoch: 0002/0003 | Batch 62750/114669 | Loss: 0.0692\n",
      "Epoch: 0002/0003 | Batch 63000/114669 | Loss: 0.1239\n",
      "Epoch: 0002/0003 | Batch 63250/114669 | Loss: 0.1399\n",
      "Epoch: 0002/0003 | Batch 63500/114669 | Loss: 0.6669\n",
      "Epoch: 0002/0003 | Batch 63750/114669 | Loss: 0.8542\n",
      "Epoch: 0002/0003 | Batch 64000/114669 | Loss: 0.1751\n",
      "Epoch: 0002/0003 | Batch 64250/114669 | Loss: 0.1857\n",
      "Epoch: 0002/0003 | Batch 64500/114669 | Loss: 0.1550\n",
      "Epoch: 0002/0003 | Batch 64750/114669 | Loss: 0.0549\n",
      "Epoch: 0002/0003 | Batch 65000/114669 | Loss: 0.2058\n",
      "Epoch: 0002/0003 | Batch 65250/114669 | Loss: 0.1180\n",
      "Epoch: 0002/0003 | Batch 65500/114669 | Loss: 0.4103\n",
      "Epoch: 0002/0003 | Batch 65750/114669 | Loss: 0.2687\n",
      "Epoch: 0002/0003 | Batch 66000/114669 | Loss: 0.8079\n",
      "Epoch: 0002/0003 | Batch 66250/114669 | Loss: 0.3686\n",
      "Epoch: 0002/0003 | Batch 66500/114669 | Loss: 0.0629\n",
      "Epoch: 0002/0003 | Batch 66750/114669 | Loss: 0.0637\n",
      "Epoch: 0002/0003 | Batch 67000/114669 | Loss: 0.0784\n",
      "Epoch: 0002/0003 | Batch 67250/114669 | Loss: 0.0916\n",
      "Epoch: 0002/0003 | Batch 67500/114669 | Loss: 0.4819\n",
      "Epoch: 0002/0003 | Batch 67750/114669 | Loss: 0.0935\n",
      "Epoch: 0002/0003 | Batch 68000/114669 | Loss: 0.1141\n",
      "Epoch: 0002/0003 | Batch 68250/114669 | Loss: 0.0941\n",
      "Epoch: 0002/0003 | Batch 68500/114669 | Loss: 0.8407\n",
      "Epoch: 0002/0003 | Batch 68750/114669 | Loss: 0.5593\n",
      "Epoch: 0002/0003 | Batch 69000/114669 | Loss: 0.1485\n",
      "Epoch: 0002/0003 | Batch 69250/114669 | Loss: 0.4797\n",
      "Epoch: 0002/0003 | Batch 69500/114669 | Loss: 0.0526\n",
      "Epoch: 0002/0003 | Batch 69750/114669 | Loss: 0.1337\n",
      "Epoch: 0002/0003 | Batch 70000/114669 | Loss: 0.0704\n",
      "Epoch: 0002/0003 | Batch 70250/114669 | Loss: 0.1404\n",
      "Epoch: 0002/0003 | Batch 70500/114669 | Loss: 0.1834\n",
      "Epoch: 0002/0003 | Batch 70750/114669 | Loss: 0.1232\n",
      "Epoch: 0002/0003 | Batch 71000/114669 | Loss: 0.1639\n",
      "Epoch: 0002/0003 | Batch 71250/114669 | Loss: 0.2509\n",
      "Epoch: 0002/0003 | Batch 71500/114669 | Loss: 0.4618\n",
      "Epoch: 0002/0003 | Batch 71750/114669 | Loss: 0.7402\n",
      "Epoch: 0002/0003 | Batch 72000/114669 | Loss: 0.0885\n",
      "Epoch: 0002/0003 | Batch 72250/114669 | Loss: 0.5437\n",
      "Epoch: 0002/0003 | Batch 72500/114669 | Loss: 0.4711\n",
      "Epoch: 0002/0003 | Batch 72750/114669 | Loss: 0.2649\n",
      "Epoch: 0002/0003 | Batch 73000/114669 | Loss: 0.4935\n",
      "Epoch: 0002/0003 | Batch 73250/114669 | Loss: 0.1476\n",
      "Epoch: 0002/0003 | Batch 73500/114669 | Loss: 0.2879\n",
      "Epoch: 0002/0003 | Batch 73750/114669 | Loss: 0.2756\n",
      "Epoch: 0002/0003 | Batch 74000/114669 | Loss: 0.3640\n",
      "Epoch: 0002/0003 | Batch 74250/114669 | Loss: 0.1163\n",
      "Epoch: 0002/0003 | Batch 74500/114669 | Loss: 0.0854\n",
      "Epoch: 0002/0003 | Batch 74750/114669 | Loss: 0.3240\n",
      "Epoch: 0002/0003 | Batch 75000/114669 | Loss: 0.6852\n",
      "Epoch: 0002/0003 | Batch 75250/114669 | Loss: 0.1147\n",
      "Epoch: 0002/0003 | Batch 75500/114669 | Loss: 0.4783\n",
      "Epoch: 0002/0003 | Batch 75750/114669 | Loss: 0.0850\n",
      "Epoch: 0002/0003 | Batch 76000/114669 | Loss: 0.4801\n",
      "Epoch: 0002/0003 | Batch 76250/114669 | Loss: 0.2731\n",
      "Epoch: 0002/0003 | Batch 76500/114669 | Loss: 0.0760\n",
      "Epoch: 0002/0003 | Batch 76750/114669 | Loss: 0.0582\n",
      "Epoch: 0002/0003 | Batch 77000/114669 | Loss: 0.3225\n",
      "Epoch: 0002/0003 | Batch 77250/114669 | Loss: 0.1183\n",
      "Epoch: 0002/0003 | Batch 77500/114669 | Loss: 0.3515\n",
      "Epoch: 0002/0003 | Batch 77750/114669 | Loss: 0.2394\n",
      "Epoch: 0002/0003 | Batch 78000/114669 | Loss: 0.2881\n",
      "Epoch: 0002/0003 | Batch 78250/114669 | Loss: 0.1321\n",
      "Epoch: 0002/0003 | Batch 78500/114669 | Loss: 0.0244\n",
      "Epoch: 0002/0003 | Batch 78750/114669 | Loss: 0.2092\n",
      "Epoch: 0002/0003 | Batch 79000/114669 | Loss: 0.2321\n",
      "Epoch: 0002/0003 | Batch 79250/114669 | Loss: 0.1045\n",
      "Epoch: 0002/0003 | Batch 79500/114669 | Loss: 0.4206\n",
      "Epoch: 0002/0003 | Batch 79750/114669 | Loss: 0.4060\n",
      "Epoch: 0002/0003 | Batch 80000/114669 | Loss: 0.3965\n",
      "Epoch: 0002/0003 | Batch 80250/114669 | Loss: 0.4929\n",
      "Epoch: 0002/0003 | Batch 80500/114669 | Loss: 0.0844\n",
      "Epoch: 0002/0003 | Batch 80750/114669 | Loss: 0.1184\n",
      "Epoch: 0002/0003 | Batch 81000/114669 | Loss: 0.0736\n",
      "Epoch: 0002/0003 | Batch 81250/114669 | Loss: 0.7136\n",
      "Epoch: 0002/0003 | Batch 81500/114669 | Loss: 0.4996\n",
      "Epoch: 0002/0003 | Batch 81750/114669 | Loss: 0.2720\n",
      "Epoch: 0002/0003 | Batch 82000/114669 | Loss: 0.3902\n",
      "Epoch: 0002/0003 | Batch 82250/114669 | Loss: 0.1751\n",
      "Epoch: 0002/0003 | Batch 82500/114669 | Loss: 0.1085\n",
      "Epoch: 0002/0003 | Batch 82750/114669 | Loss: 0.1170\n",
      "Epoch: 0002/0003 | Batch 83000/114669 | Loss: 0.1443\n",
      "Epoch: 0002/0003 | Batch 83250/114669 | Loss: 0.8809\n",
      "Epoch: 0002/0003 | Batch 83500/114669 | Loss: 0.5806\n",
      "Epoch: 0002/0003 | Batch 83750/114669 | Loss: 0.1720\n",
      "Epoch: 0002/0003 | Batch 84000/114669 | Loss: 0.3832\n",
      "Epoch: 0002/0003 | Batch 84250/114669 | Loss: 0.6085\n",
      "Epoch: 0002/0003 | Batch 84500/114669 | Loss: 0.2372\n",
      "Epoch: 0002/0003 | Batch 84750/114669 | Loss: 0.3856\n",
      "Epoch: 0002/0003 | Batch 85000/114669 | Loss: 0.7914\n",
      "Epoch: 0002/0003 | Batch 85250/114669 | Loss: 0.1424\n",
      "Epoch: 0002/0003 | Batch 85500/114669 | Loss: 1.1314\n",
      "Epoch: 0002/0003 | Batch 85750/114669 | Loss: 0.0753\n",
      "Epoch: 0002/0003 | Batch 86000/114669 | Loss: 0.2857\n",
      "Epoch: 0002/0003 | Batch 86250/114669 | Loss: 0.3183\n",
      "Epoch: 0002/0003 | Batch 86500/114669 | Loss: 0.0790\n",
      "Epoch: 0002/0003 | Batch 86750/114669 | Loss: 0.1300\n",
      "Epoch: 0002/0003 | Batch 87000/114669 | Loss: 0.3137\n",
      "Epoch: 0002/0003 | Batch 87250/114669 | Loss: 0.1622\n",
      "Epoch: 0002/0003 | Batch 87500/114669 | Loss: 0.3747\n",
      "Epoch: 0002/0003 | Batch 87750/114669 | Loss: 0.5479\n",
      "Epoch: 0002/0003 | Batch 88000/114669 | Loss: 0.0845\n",
      "Epoch: 0002/0003 | Batch 88250/114669 | Loss: 0.0383\n",
      "Epoch: 0002/0003 | Batch 88500/114669 | Loss: 0.2210\n",
      "Epoch: 0002/0003 | Batch 88750/114669 | Loss: 0.7859\n",
      "Epoch: 0002/0003 | Batch 89000/114669 | Loss: 0.3786\n",
      "Epoch: 0002/0003 | Batch 89250/114669 | Loss: 0.1642\n",
      "Epoch: 0002/0003 | Batch 89500/114669 | Loss: 0.0372\n",
      "Epoch: 0002/0003 | Batch 89750/114669 | Loss: 0.1289\n",
      "Epoch: 0002/0003 | Batch 90000/114669 | Loss: 0.3228\n",
      "Epoch: 0002/0003 | Batch 90250/114669 | Loss: 0.6374\n",
      "Epoch: 0002/0003 | Batch 90500/114669 | Loss: 0.2908\n",
      "Epoch: 0002/0003 | Batch 90750/114669 | Loss: 0.2588\n",
      "Epoch: 0002/0003 | Batch 91000/114669 | Loss: 0.2894\n",
      "Epoch: 0002/0003 | Batch 91250/114669 | Loss: 0.0629\n",
      "Epoch: 0002/0003 | Batch 91500/114669 | Loss: 0.5142\n",
      "Epoch: 0002/0003 | Batch 91750/114669 | Loss: 0.5220\n",
      "Epoch: 0002/0003 | Batch 92000/114669 | Loss: 0.0973\n",
      "Epoch: 0002/0003 | Batch 92250/114669 | Loss: 0.0592\n",
      "Epoch: 0002/0003 | Batch 92500/114669 | Loss: 0.1822\n",
      "Epoch: 0002/0003 | Batch 92750/114669 | Loss: 0.3843\n",
      "Epoch: 0002/0003 | Batch 93000/114669 | Loss: 0.5035\n",
      "Epoch: 0002/0003 | Batch 93250/114669 | Loss: 0.7659\n",
      "Epoch: 0002/0003 | Batch 93500/114669 | Loss: 0.1034\n",
      "Epoch: 0002/0003 | Batch 93750/114669 | Loss: 0.1114\n",
      "Epoch: 0002/0003 | Batch 94000/114669 | Loss: 0.4086\n",
      "Epoch: 0002/0003 | Batch 94250/114669 | Loss: 0.1038\n",
      "Epoch: 0002/0003 | Batch 94500/114669 | Loss: 1.0450\n",
      "Epoch: 0002/0003 | Batch 94750/114669 | Loss: 0.2290\n",
      "Epoch: 0002/0003 | Batch 95000/114669 | Loss: 0.4175\n",
      "Epoch: 0002/0003 | Batch 95250/114669 | Loss: 0.1410\n",
      "Epoch: 0002/0003 | Batch 95500/114669 | Loss: 0.6443\n",
      "Epoch: 0002/0003 | Batch 95750/114669 | Loss: 0.0417\n",
      "Epoch: 0002/0003 | Batch 96000/114669 | Loss: 0.2420\n",
      "Epoch: 0002/0003 | Batch 96250/114669 | Loss: 0.1181\n",
      "Epoch: 0002/0003 | Batch 96500/114669 | Loss: 0.0802\n",
      "Epoch: 0002/0003 | Batch 96750/114669 | Loss: 0.1016\n",
      "Epoch: 0002/0003 | Batch 97000/114669 | Loss: 0.1271\n",
      "Epoch: 0002/0003 | Batch 97250/114669 | Loss: 0.0408\n",
      "Epoch: 0002/0003 | Batch 97500/114669 | Loss: 0.4979\n",
      "Epoch: 0002/0003 | Batch 97750/114669 | Loss: 0.1532\n",
      "Epoch: 0002/0003 | Batch 98000/114669 | Loss: 0.1246\n",
      "Epoch: 0002/0003 | Batch 98250/114669 | Loss: 0.4204\n",
      "Epoch: 0002/0003 | Batch 98500/114669 | Loss: 0.6072\n",
      "Epoch: 0002/0003 | Batch 98750/114669 | Loss: 0.0891\n",
      "Epoch: 0002/0003 | Batch 99000/114669 | Loss: 0.0214\n",
      "Epoch: 0002/0003 | Batch 99250/114669 | Loss: 0.5873\n",
      "Epoch: 0002/0003 | Batch 99500/114669 | Loss: 0.7325\n",
      "Epoch: 0002/0003 | Batch 99750/114669 | Loss: 0.0589\n",
      "Epoch: 0002/0003 | Batch 100000/114669 | Loss: 0.5080\n",
      "Epoch: 0002/0003 | Batch 100250/114669 | Loss: 0.3973\n",
      "Epoch: 0002/0003 | Batch 100500/114669 | Loss: 0.2144\n",
      "Epoch: 0002/0003 | Batch 100750/114669 | Loss: 0.2797\n",
      "Epoch: 0002/0003 | Batch 101000/114669 | Loss: 0.2948\n",
      "Epoch: 0002/0003 | Batch 101250/114669 | Loss: 0.1254\n",
      "Epoch: 0002/0003 | Batch 101500/114669 | Loss: 0.5233\n",
      "Epoch: 0002/0003 | Batch 101750/114669 | Loss: 0.2367\n",
      "Epoch: 0002/0003 | Batch 102000/114669 | Loss: 0.1093\n",
      "Epoch: 0002/0003 | Batch 102250/114669 | Loss: 0.0853\n",
      "Epoch: 0002/0003 | Batch 102500/114669 | Loss: 0.3036\n",
      "Epoch: 0002/0003 | Batch 102750/114669 | Loss: 0.3591\n",
      "Epoch: 0002/0003 | Batch 103000/114669 | Loss: 0.1621\n",
      "Epoch: 0002/0003 | Batch 103250/114669 | Loss: 0.3035\n",
      "Epoch: 0002/0003 | Batch 103500/114669 | Loss: 0.2328\n",
      "Epoch: 0002/0003 | Batch 103750/114669 | Loss: 0.5188\n",
      "Epoch: 0002/0003 | Batch 104000/114669 | Loss: 0.0569\n",
      "Epoch: 0002/0003 | Batch 104250/114669 | Loss: 0.0821\n",
      "Epoch: 0002/0003 | Batch 104500/114669 | Loss: 0.2752\n",
      "Epoch: 0002/0003 | Batch 104750/114669 | Loss: 0.1384\n",
      "Epoch: 0002/0003 | Batch 105000/114669 | Loss: 0.2279\n",
      "Epoch: 0002/0003 | Batch 105250/114669 | Loss: 0.1372\n",
      "Epoch: 0002/0003 | Batch 105500/114669 | Loss: 0.0594\n",
      "Epoch: 0002/0003 | Batch 105750/114669 | Loss: 0.0516\n",
      "Epoch: 0002/0003 | Batch 106000/114669 | Loss: 0.2193\n",
      "Epoch: 0002/0003 | Batch 106250/114669 | Loss: 0.5888\n",
      "Epoch: 0002/0003 | Batch 106500/114669 | Loss: 0.1128\n",
      "Epoch: 0002/0003 | Batch 106750/114669 | Loss: 0.4464\n",
      "Epoch: 0002/0003 | Batch 107000/114669 | Loss: 0.4670\n",
      "Epoch: 0002/0003 | Batch 107250/114669 | Loss: 0.0559\n",
      "Epoch: 0002/0003 | Batch 107500/114669 | Loss: 0.0741\n",
      "Epoch: 0002/0003 | Batch 107750/114669 | Loss: 0.3513\n",
      "Epoch: 0002/0003 | Batch 108000/114669 | Loss: 0.2203\n",
      "Epoch: 0002/0003 | Batch 108250/114669 | Loss: 0.1592\n",
      "Epoch: 0002/0003 | Batch 108500/114669 | Loss: 0.5424\n",
      "Epoch: 0002/0003 | Batch 108750/114669 | Loss: 0.1811\n",
      "Epoch: 0002/0003 | Batch 109000/114669 | Loss: 0.1155\n",
      "Epoch: 0002/0003 | Batch 109250/114669 | Loss: 0.3065\n",
      "Epoch: 0002/0003 | Batch 109500/114669 | Loss: 0.0836\n",
      "Epoch: 0002/0003 | Batch 109750/114669 | Loss: 0.3894\n",
      "Epoch: 0002/0003 | Batch 110000/114669 | Loss: 0.4167\n",
      "Epoch: 0002/0003 | Batch 110250/114669 | Loss: 1.1562\n",
      "Epoch: 0002/0003 | Batch 110500/114669 | Loss: 0.2894\n",
      "Epoch: 0002/0003 | Batch 110750/114669 | Loss: 0.1854\n",
      "Epoch: 0002/0003 | Batch 111000/114669 | Loss: 0.4809\n",
      "Epoch: 0002/0003 | Batch 111250/114669 | Loss: 0.0392\n",
      "Epoch: 0002/0003 | Batch 111500/114669 | Loss: 0.5655\n",
      "Epoch: 0002/0003 | Batch 111750/114669 | Loss: 0.0846\n",
      "Epoch: 0002/0003 | Batch 112000/114669 | Loss: 0.2141\n",
      "Epoch: 0002/0003 | Batch 112250/114669 | Loss: 0.2707\n",
      "Epoch: 0002/0003 | Batch 112500/114669 | Loss: 0.1615\n",
      "Epoch: 0002/0003 | Batch 112750/114669 | Loss: 0.0518\n",
      "Epoch: 0002/0003 | Batch 113000/114669 | Loss: 0.2327\n",
      "Epoch: 0002/0003 | Batch 113250/114669 | Loss: 0.2537\n",
      "Epoch: 0002/0003 | Batch 113500/114669 | Loss: 0.0580\n",
      "Epoch: 0002/0003 | Batch 113750/114669 | Loss: 0.4211\n",
      "Epoch: 0002/0003 | Batch 114000/114669 | Loss: 0.0755\n",
      "Epoch: 0002/0003 | Batch 114250/114669 | Loss: 0.0934\n",
      "Epoch: 0002/0003 | Batch 114500/114669 | Loss: 0.8468\n",
      "Training accuracy: 31.91%\n",
      "Valid accuracy: 31.83%\n",
      "Time elapsed: 186.52 min\n",
      "Epoch: 0003/0003 | Batch 0000/114669 | Loss: 0.3255\n",
      "Epoch: 0003/0003 | Batch 0250/114669 | Loss: 0.4602\n",
      "Epoch: 0003/0003 | Batch 0500/114669 | Loss: 0.3102\n",
      "Epoch: 0003/0003 | Batch 0750/114669 | Loss: 0.0767\n",
      "Epoch: 0003/0003 | Batch 1000/114669 | Loss: 0.0512\n",
      "Epoch: 0003/0003 | Batch 1250/114669 | Loss: 0.0877\n",
      "Epoch: 0003/0003 | Batch 1500/114669 | Loss: 0.7001\n",
      "Epoch: 0003/0003 | Batch 1750/114669 | Loss: 0.3922\n",
      "Epoch: 0003/0003 | Batch 2000/114669 | Loss: 0.4066\n",
      "Epoch: 0003/0003 | Batch 2250/114669 | Loss: 0.2980\n",
      "Epoch: 0003/0003 | Batch 2500/114669 | Loss: 0.6542\n",
      "Epoch: 0003/0003 | Batch 2750/114669 | Loss: 0.2978\n",
      "Epoch: 0003/0003 | Batch 3000/114669 | Loss: 0.1049\n",
      "Epoch: 0003/0003 | Batch 3250/114669 | Loss: 0.2228\n",
      "Epoch: 0003/0003 | Batch 3500/114669 | Loss: 0.1057\n",
      "Epoch: 0003/0003 | Batch 3750/114669 | Loss: 0.3377\n",
      "Epoch: 0003/0003 | Batch 4000/114669 | Loss: 0.3498\n",
      "Epoch: 0003/0003 | Batch 4250/114669 | Loss: 0.0906\n",
      "Epoch: 0003/0003 | Batch 4500/114669 | Loss: 0.0595\n",
      "Epoch: 0003/0003 | Batch 4750/114669 | Loss: 0.3955\n",
      "Epoch: 0003/0003 | Batch 5000/114669 | Loss: 0.3651\n",
      "Epoch: 0003/0003 | Batch 5250/114669 | Loss: 0.3982\n",
      "Epoch: 0003/0003 | Batch 5500/114669 | Loss: 1.0469\n",
      "Epoch: 0003/0003 | Batch 5750/114669 | Loss: 0.1016\n",
      "Epoch: 0003/0003 | Batch 6000/114669 | Loss: 0.7661\n",
      "Epoch: 0003/0003 | Batch 6250/114669 | Loss: 0.3189\n",
      "Epoch: 0003/0003 | Batch 6500/114669 | Loss: 0.0624\n",
      "Epoch: 0003/0003 | Batch 6750/114669 | Loss: 0.3557\n",
      "Epoch: 0003/0003 | Batch 7000/114669 | Loss: 0.1458\n",
      "Epoch: 0003/0003 | Batch 7250/114669 | Loss: 0.0879\n",
      "Epoch: 0003/0003 | Batch 7500/114669 | Loss: 0.0718\n",
      "Epoch: 0003/0003 | Batch 7750/114669 | Loss: 0.0571\n",
      "Epoch: 0003/0003 | Batch 8000/114669 | Loss: 0.0382\n",
      "Epoch: 0003/0003 | Batch 8250/114669 | Loss: 0.6290\n",
      "Epoch: 0003/0003 | Batch 8500/114669 | Loss: 0.2935\n",
      "Epoch: 0003/0003 | Batch 8750/114669 | Loss: 0.2047\n",
      "Epoch: 0003/0003 | Batch 9000/114669 | Loss: 0.3149\n",
      "Epoch: 0003/0003 | Batch 9250/114669 | Loss: 0.1187\n",
      "Epoch: 0003/0003 | Batch 9500/114669 | Loss: 0.1022\n",
      "Epoch: 0003/0003 | Batch 9750/114669 | Loss: 0.3034\n",
      "Epoch: 0003/0003 | Batch 10000/114669 | Loss: 0.4081\n",
      "Epoch: 0003/0003 | Batch 10250/114669 | Loss: 0.0718\n",
      "Epoch: 0003/0003 | Batch 10500/114669 | Loss: 0.5510\n",
      "Epoch: 0003/0003 | Batch 10750/114669 | Loss: 0.3363\n",
      "Epoch: 0003/0003 | Batch 11000/114669 | Loss: 0.0647\n",
      "Epoch: 0003/0003 | Batch 11250/114669 | Loss: 0.1025\n",
      "Epoch: 0003/0003 | Batch 11500/114669 | Loss: 0.0584\n",
      "Epoch: 0003/0003 | Batch 11750/114669 | Loss: 0.0810\n",
      "Epoch: 0003/0003 | Batch 12000/114669 | Loss: 0.0521\n",
      "Epoch: 0003/0003 | Batch 12250/114669 | Loss: 0.5903\n",
      "Epoch: 0003/0003 | Batch 12500/114669 | Loss: 0.0725\n",
      "Epoch: 0003/0003 | Batch 12750/114669 | Loss: 0.3028\n",
      "Epoch: 0003/0003 | Batch 13000/114669 | Loss: 0.5078\n",
      "Epoch: 0003/0003 | Batch 13250/114669 | Loss: 0.3225\n",
      "Epoch: 0003/0003 | Batch 13500/114669 | Loss: 0.3052\n",
      "Epoch: 0003/0003 | Batch 13750/114669 | Loss: 0.0651\n",
      "Epoch: 0003/0003 | Batch 14000/114669 | Loss: 0.5860\n",
      "Epoch: 0003/0003 | Batch 14250/114669 | Loss: 0.0526\n",
      "Epoch: 0003/0003 | Batch 14500/114669 | Loss: 0.2696\n",
      "Epoch: 0003/0003 | Batch 14750/114669 | Loss: 0.1793\n",
      "Epoch: 0003/0003 | Batch 15000/114669 | Loss: 0.8212\n",
      "Epoch: 0003/0003 | Batch 15250/114669 | Loss: 0.1129\n",
      "Epoch: 0003/0003 | Batch 15500/114669 | Loss: 0.4470\n",
      "Epoch: 0003/0003 | Batch 15750/114669 | Loss: 0.1238\n",
      "Epoch: 0003/0003 | Batch 16000/114669 | Loss: 0.3392\n",
      "Epoch: 0003/0003 | Batch 16250/114669 | Loss: 0.2493\n",
      "Epoch: 0003/0003 | Batch 16500/114669 | Loss: 0.0439\n",
      "Epoch: 0003/0003 | Batch 16750/114669 | Loss: 0.0316\n",
      "Epoch: 0003/0003 | Batch 17000/114669 | Loss: 0.2341\n",
      "Epoch: 0003/0003 | Batch 17250/114669 | Loss: 0.0926\n",
      "Epoch: 0003/0003 | Batch 17500/114669 | Loss: 0.2602\n",
      "Epoch: 0003/0003 | Batch 17750/114669 | Loss: 0.3792\n",
      "Epoch: 0003/0003 | Batch 18000/114669 | Loss: 0.2089\n",
      "Epoch: 0003/0003 | Batch 18250/114669 | Loss: 0.2524\n",
      "Epoch: 0003/0003 | Batch 18500/114669 | Loss: 0.3624\n",
      "Epoch: 0003/0003 | Batch 18750/114669 | Loss: 0.2240\n",
      "Epoch: 0003/0003 | Batch 19000/114669 | Loss: 0.1263\n",
      "Epoch: 0003/0003 | Batch 19250/114669 | Loss: 0.4392\n",
      "Epoch: 0003/0003 | Batch 19500/114669 | Loss: 0.3113\n",
      "Epoch: 0003/0003 | Batch 19750/114669 | Loss: 0.0327\n",
      "Epoch: 0003/0003 | Batch 20000/114669 | Loss: 0.1056\n",
      "Epoch: 0003/0003 | Batch 20250/114669 | Loss: 0.0592\n",
      "Epoch: 0003/0003 | Batch 20500/114669 | Loss: 0.6041\n",
      "Epoch: 0003/0003 | Batch 20750/114669 | Loss: 0.1218\n",
      "Epoch: 0003/0003 | Batch 21000/114669 | Loss: 0.9318\n",
      "Epoch: 0003/0003 | Batch 21250/114669 | Loss: 0.8467\n",
      "Epoch: 0003/0003 | Batch 21500/114669 | Loss: 0.0668\n",
      "Epoch: 0003/0003 | Batch 21750/114669 | Loss: 0.1362\n",
      "Epoch: 0003/0003 | Batch 22000/114669 | Loss: 0.2113\n",
      "Epoch: 0003/0003 | Batch 22250/114669 | Loss: 0.0960\n",
      "Epoch: 0003/0003 | Batch 22500/114669 | Loss: 0.3033\n",
      "Epoch: 0003/0003 | Batch 22750/114669 | Loss: 0.2833\n",
      "Epoch: 0003/0003 | Batch 23000/114669 | Loss: 0.1122\n",
      "Epoch: 0003/0003 | Batch 23250/114669 | Loss: 0.2009\n",
      "Epoch: 0003/0003 | Batch 23500/114669 | Loss: 0.4925\n",
      "Epoch: 0003/0003 | Batch 23750/114669 | Loss: 0.3177\n",
      "Epoch: 0003/0003 | Batch 24000/114669 | Loss: 0.0739\n",
      "Epoch: 0003/0003 | Batch 24250/114669 | Loss: 0.1069\n",
      "Epoch: 0003/0003 | Batch 24500/114669 | Loss: 0.1659\n",
      "Epoch: 0003/0003 | Batch 24750/114669 | Loss: 0.1585\n",
      "Epoch: 0003/0003 | Batch 25000/114669 | Loss: 0.7932\n",
      "Epoch: 0003/0003 | Batch 25250/114669 | Loss: 0.4380\n",
      "Epoch: 0003/0003 | Batch 25500/114669 | Loss: 0.0810\n",
      "Epoch: 0003/0003 | Batch 25750/114669 | Loss: 0.2428\n",
      "Epoch: 0003/0003 | Batch 26000/114669 | Loss: 0.3259\n",
      "Epoch: 0003/0003 | Batch 26250/114669 | Loss: 0.3981\n",
      "Epoch: 0003/0003 | Batch 26500/114669 | Loss: 0.3120\n",
      "Epoch: 0003/0003 | Batch 26750/114669 | Loss: 0.3474\n",
      "Epoch: 0003/0003 | Batch 27000/114669 | Loss: 0.3678\n",
      "Epoch: 0003/0003 | Batch 27250/114669 | Loss: 0.0615\n",
      "Epoch: 0003/0003 | Batch 27500/114669 | Loss: 0.1184\n",
      "Epoch: 0003/0003 | Batch 27750/114669 | Loss: 0.0511\n",
      "Epoch: 0003/0003 | Batch 28000/114669 | Loss: 0.3889\n",
      "Epoch: 0003/0003 | Batch 28250/114669 | Loss: 0.0784\n",
      "Epoch: 0003/0003 | Batch 28500/114669 | Loss: 0.2551\n",
      "Epoch: 0003/0003 | Batch 28750/114669 | Loss: 0.2075\n",
      "Epoch: 0003/0003 | Batch 29000/114669 | Loss: 0.5231\n",
      "Epoch: 0003/0003 | Batch 29250/114669 | Loss: 0.2727\n",
      "Epoch: 0003/0003 | Batch 29500/114669 | Loss: 0.4273\n",
      "Epoch: 0003/0003 | Batch 29750/114669 | Loss: 0.0378\n",
      "Epoch: 0003/0003 | Batch 30000/114669 | Loss: 0.5877\n",
      "Epoch: 0003/0003 | Batch 30250/114669 | Loss: 0.4896\n",
      "Epoch: 0003/0003 | Batch 30500/114669 | Loss: 0.1268\n",
      "Epoch: 0003/0003 | Batch 30750/114669 | Loss: 0.7014\n",
      "Epoch: 0003/0003 | Batch 31000/114669 | Loss: 0.2741\n",
      "Epoch: 0003/0003 | Batch 31250/114669 | Loss: 0.3035\n",
      "Epoch: 0003/0003 | Batch 31500/114669 | Loss: 0.2477\n",
      "Epoch: 0003/0003 | Batch 31750/114669 | Loss: 0.0992\n",
      "Epoch: 0003/0003 | Batch 32000/114669 | Loss: 0.3922\n",
      "Epoch: 0003/0003 | Batch 32250/114669 | Loss: 0.5494\n",
      "Epoch: 0003/0003 | Batch 32500/114669 | Loss: 0.3282\n",
      "Epoch: 0003/0003 | Batch 32750/114669 | Loss: 0.4985\n",
      "Epoch: 0003/0003 | Batch 33000/114669 | Loss: 0.2486\n",
      "Epoch: 0003/0003 | Batch 33250/114669 | Loss: 0.1116\n",
      "Epoch: 0003/0003 | Batch 33500/114669 | Loss: 0.5280\n",
      "Epoch: 0003/0003 | Batch 33750/114669 | Loss: 0.1086\n",
      "Epoch: 0003/0003 | Batch 34000/114669 | Loss: 0.1222\n",
      "Epoch: 0003/0003 | Batch 34250/114669 | Loss: 0.0221\n",
      "Epoch: 0003/0003 | Batch 34500/114669 | Loss: 0.1654\n",
      "Epoch: 0003/0003 | Batch 34750/114669 | Loss: 0.4867\n",
      "Epoch: 0003/0003 | Batch 35000/114669 | Loss: 0.2495\n",
      "Epoch: 0003/0003 | Batch 35250/114669 | Loss: 0.0524\n",
      "Epoch: 0003/0003 | Batch 35500/114669 | Loss: 0.5959\n",
      "Epoch: 0003/0003 | Batch 35750/114669 | Loss: 0.3635\n",
      "Epoch: 0003/0003 | Batch 36000/114669 | Loss: 0.0822\n",
      "Epoch: 0003/0003 | Batch 36250/114669 | Loss: 0.4141\n",
      "Epoch: 0003/0003 | Batch 36500/114669 | Loss: 0.1127\n",
      "Epoch: 0003/0003 | Batch 36750/114669 | Loss: 0.1048\n",
      "Epoch: 0003/0003 | Batch 37000/114669 | Loss: 0.0458\n",
      "Epoch: 0003/0003 | Batch 37250/114669 | Loss: 0.1456\n",
      "Epoch: 0003/0003 | Batch 37500/114669 | Loss: 0.1291\n",
      "Epoch: 0003/0003 | Batch 37750/114669 | Loss: 0.0997\n",
      "Epoch: 0003/0003 | Batch 38000/114669 | Loss: 0.1384\n",
      "Epoch: 0003/0003 | Batch 38250/114669 | Loss: 0.2493\n",
      "Epoch: 0003/0003 | Batch 38500/114669 | Loss: 0.4902\n",
      "Epoch: 0003/0003 | Batch 38750/114669 | Loss: 0.0830\n",
      "Epoch: 0003/0003 | Batch 39000/114669 | Loss: 0.0946\n",
      "Epoch: 0003/0003 | Batch 39250/114669 | Loss: 0.4372\n",
      "Epoch: 0003/0003 | Batch 39500/114669 | Loss: 0.3034\n",
      "Epoch: 0003/0003 | Batch 39750/114669 | Loss: 0.0756\n",
      "Epoch: 0003/0003 | Batch 40000/114669 | Loss: 0.0685\n",
      "Epoch: 0003/0003 | Batch 40250/114669 | Loss: 0.3416\n",
      "Epoch: 0003/0003 | Batch 40500/114669 | Loss: 0.1624\n",
      "Epoch: 0003/0003 | Batch 40750/114669 | Loss: 0.1293\n",
      "Epoch: 0003/0003 | Batch 41000/114669 | Loss: 0.4466\n",
      "Epoch: 0003/0003 | Batch 41250/114669 | Loss: 0.3082\n",
      "Epoch: 0003/0003 | Batch 41500/114669 | Loss: 0.0402\n",
      "Epoch: 0003/0003 | Batch 41750/114669 | Loss: 0.0781\n",
      "Epoch: 0003/0003 | Batch 42000/114669 | Loss: 0.1084\n",
      "Epoch: 0003/0003 | Batch 42250/114669 | Loss: 0.2124\n",
      "Epoch: 0003/0003 | Batch 42500/114669 | Loss: 0.0883\n",
      "Epoch: 0003/0003 | Batch 42750/114669 | Loss: 0.1530\n",
      "Epoch: 0003/0003 | Batch 43000/114669 | Loss: 0.5605\n",
      "Epoch: 0003/0003 | Batch 43250/114669 | Loss: 0.1641\n",
      "Epoch: 0003/0003 | Batch 43500/114669 | Loss: 0.2214\n",
      "Epoch: 0003/0003 | Batch 43750/114669 | Loss: 0.0550\n",
      "Epoch: 0003/0003 | Batch 44000/114669 | Loss: 0.3984\n",
      "Epoch: 0003/0003 | Batch 44250/114669 | Loss: 0.0607\n",
      "Epoch: 0003/0003 | Batch 44500/114669 | Loss: 0.3518\n",
      "Epoch: 0003/0003 | Batch 44750/114669 | Loss: 0.0759\n",
      "Epoch: 0003/0003 | Batch 45000/114669 | Loss: 0.1218\n",
      "Epoch: 0003/0003 | Batch 45250/114669 | Loss: 0.8325\n",
      "Epoch: 0003/0003 | Batch 45500/114669 | Loss: 0.1084\n",
      "Epoch: 0003/0003 | Batch 45750/114669 | Loss: 0.1128\n",
      "Epoch: 0003/0003 | Batch 46000/114669 | Loss: 0.3847\n",
      "Epoch: 0003/0003 | Batch 46250/114669 | Loss: 0.0691\n",
      "Epoch: 0003/0003 | Batch 46500/114669 | Loss: 0.0848\n",
      "Epoch: 0003/0003 | Batch 46750/114669 | Loss: 0.0610\n",
      "Epoch: 0003/0003 | Batch 47000/114669 | Loss: 0.1658\n",
      "Epoch: 0003/0003 | Batch 47250/114669 | Loss: 0.0523\n",
      "Epoch: 0003/0003 | Batch 47500/114669 | Loss: 1.0479\n",
      "Epoch: 0003/0003 | Batch 47750/114669 | Loss: 0.1157\n",
      "Epoch: 0003/0003 | Batch 48000/114669 | Loss: 0.4432\n",
      "Epoch: 0003/0003 | Batch 48250/114669 | Loss: 0.1311\n",
      "Epoch: 0003/0003 | Batch 48500/114669 | Loss: 0.0747\n",
      "Epoch: 0003/0003 | Batch 48750/114669 | Loss: 0.3279\n",
      "Epoch: 0003/0003 | Batch 49000/114669 | Loss: 0.3534\n",
      "Epoch: 0003/0003 | Batch 49250/114669 | Loss: 0.1980\n",
      "Epoch: 0003/0003 | Batch 49500/114669 | Loss: 0.0799\n",
      "Epoch: 0003/0003 | Batch 49750/114669 | Loss: 0.2205\n",
      "Epoch: 0003/0003 | Batch 50000/114669 | Loss: 0.2304\n",
      "Epoch: 0003/0003 | Batch 50250/114669 | Loss: 0.1074\n",
      "Epoch: 0003/0003 | Batch 50500/114669 | Loss: 0.8595\n",
      "Epoch: 0003/0003 | Batch 50750/114669 | Loss: 0.0766\n",
      "Epoch: 0003/0003 | Batch 51000/114669 | Loss: 0.0512\n",
      "Epoch: 0003/0003 | Batch 51250/114669 | Loss: 0.3176\n",
      "Epoch: 0003/0003 | Batch 51500/114669 | Loss: 0.2360\n",
      "Epoch: 0003/0003 | Batch 51750/114669 | Loss: 0.3230\n",
      "Epoch: 0003/0003 | Batch 52000/114669 | Loss: 0.7701\n",
      "Epoch: 0003/0003 | Batch 52250/114669 | Loss: 0.7010\n",
      "Epoch: 0003/0003 | Batch 52500/114669 | Loss: 0.1187\n",
      "Epoch: 0003/0003 | Batch 52750/114669 | Loss: 0.1686\n",
      "Epoch: 0003/0003 | Batch 53000/114669 | Loss: 0.1939\n",
      "Epoch: 0003/0003 | Batch 53250/114669 | Loss: 0.0971\n",
      "Epoch: 0003/0003 | Batch 53500/114669 | Loss: 0.3689\n",
      "Epoch: 0003/0003 | Batch 53750/114669 | Loss: 0.1540\n",
      "Epoch: 0003/0003 | Batch 54000/114669 | Loss: 0.0612\n",
      "Epoch: 0003/0003 | Batch 54250/114669 | Loss: 0.1212\n",
      "Epoch: 0003/0003 | Batch 54500/114669 | Loss: 0.1220\n",
      "Epoch: 0003/0003 | Batch 54750/114669 | Loss: 0.3166\n",
      "Epoch: 0003/0003 | Batch 55000/114669 | Loss: 0.5415\n",
      "Epoch: 0003/0003 | Batch 55250/114669 | Loss: 0.4999\n",
      "Epoch: 0003/0003 | Batch 55500/114669 | Loss: 0.6272\n",
      "Epoch: 0003/0003 | Batch 55750/114669 | Loss: 0.5888\n",
      "Epoch: 0003/0003 | Batch 56000/114669 | Loss: 0.2869\n",
      "Epoch: 0003/0003 | Batch 56250/114669 | Loss: 0.3234\n",
      "Epoch: 0003/0003 | Batch 56500/114669 | Loss: 0.2014\n",
      "Epoch: 0003/0003 | Batch 56750/114669 | Loss: 0.0657\n",
      "Epoch: 0003/0003 | Batch 57000/114669 | Loss: 0.1572\n",
      "Epoch: 0003/0003 | Batch 57250/114669 | Loss: 0.3700\n",
      "Epoch: 0003/0003 | Batch 57500/114669 | Loss: 0.2133\n",
      "Epoch: 0003/0003 | Batch 57750/114669 | Loss: 0.2906\n",
      "Epoch: 0003/0003 | Batch 58000/114669 | Loss: 0.0833\n",
      "Epoch: 0003/0003 | Batch 58250/114669 | Loss: 0.1711\n",
      "Epoch: 0003/0003 | Batch 58500/114669 | Loss: 0.1551\n",
      "Epoch: 0003/0003 | Batch 58750/114669 | Loss: 0.7938\n",
      "Epoch: 0003/0003 | Batch 59000/114669 | Loss: 0.1538\n",
      "Epoch: 0003/0003 | Batch 59250/114669 | Loss: 0.2223\n",
      "Epoch: 0003/0003 | Batch 59500/114669 | Loss: 0.0855\n",
      "Epoch: 0003/0003 | Batch 59750/114669 | Loss: 0.8729\n",
      "Epoch: 0003/0003 | Batch 60000/114669 | Loss: 0.2313\n",
      "Epoch: 0003/0003 | Batch 60250/114669 | Loss: 1.0186\n",
      "Epoch: 0003/0003 | Batch 60500/114669 | Loss: 0.1935\n",
      "Epoch: 0003/0003 | Batch 60750/114669 | Loss: 0.0834\n",
      "Epoch: 0003/0003 | Batch 61000/114669 | Loss: 0.4033\n",
      "Epoch: 0003/0003 | Batch 61250/114669 | Loss: 0.4477\n",
      "Epoch: 0003/0003 | Batch 61500/114669 | Loss: 0.1934\n",
      "Epoch: 0003/0003 | Batch 61750/114669 | Loss: 0.2332\n",
      "Epoch: 0003/0003 | Batch 62000/114669 | Loss: 0.0529\n",
      "Epoch: 0003/0003 | Batch 62250/114669 | Loss: 0.2204\n",
      "Epoch: 0003/0003 | Batch 62500/114669 | Loss: 0.2473\n",
      "Epoch: 0003/0003 | Batch 62750/114669 | Loss: 0.0694\n",
      "Epoch: 0003/0003 | Batch 63000/114669 | Loss: 0.1842\n",
      "Epoch: 0003/0003 | Batch 63250/114669 | Loss: 0.4823\n",
      "Epoch: 0003/0003 | Batch 63500/114669 | Loss: 0.7668\n",
      "Epoch: 0003/0003 | Batch 63750/114669 | Loss: 0.2520\n",
      "Epoch: 0003/0003 | Batch 64000/114669 | Loss: 0.2593\n",
      "Epoch: 0003/0003 | Batch 64250/114669 | Loss: 0.2042\n",
      "Epoch: 0003/0003 | Batch 64500/114669 | Loss: 0.0513\n",
      "Epoch: 0003/0003 | Batch 64750/114669 | Loss: 0.0660\n",
      "Epoch: 0003/0003 | Batch 65000/114669 | Loss: 0.2483\n",
      "Epoch: 0003/0003 | Batch 65250/114669 | Loss: 0.0693\n",
      "Epoch: 0003/0003 | Batch 65500/114669 | Loss: 0.0547\n",
      "Epoch: 0003/0003 | Batch 65750/114669 | Loss: 0.2024\n",
      "Epoch: 0003/0003 | Batch 66000/114669 | Loss: 0.4675\n",
      "Epoch: 0003/0003 | Batch 66250/114669 | Loss: 0.0941\n",
      "Epoch: 0003/0003 | Batch 66500/114669 | Loss: 0.6398\n",
      "Epoch: 0003/0003 | Batch 66750/114669 | Loss: 0.1757\n",
      "Epoch: 0003/0003 | Batch 67000/114669 | Loss: 0.0626\n",
      "Epoch: 0003/0003 | Batch 67250/114669 | Loss: 0.2660\n",
      "Epoch: 0003/0003 | Batch 67500/114669 | Loss: 0.0653\n",
      "Epoch: 0003/0003 | Batch 67750/114669 | Loss: 0.3626\n",
      "Epoch: 0003/0003 | Batch 68000/114669 | Loss: 0.0773\n",
      "Epoch: 0003/0003 | Batch 68250/114669 | Loss: 0.4142\n",
      "Epoch: 0003/0003 | Batch 68500/114669 | Loss: 0.2104\n",
      "Epoch: 0003/0003 | Batch 68750/114669 | Loss: 0.2750\n",
      "Epoch: 0003/0003 | Batch 69000/114669 | Loss: 0.4662\n",
      "Epoch: 0003/0003 | Batch 69250/114669 | Loss: 0.3919\n",
      "Epoch: 0003/0003 | Batch 69500/114669 | Loss: 0.4534\n",
      "Epoch: 0003/0003 | Batch 69750/114669 | Loss: 0.4232\n",
      "Epoch: 0003/0003 | Batch 70000/114669 | Loss: 0.3176\n",
      "Epoch: 0003/0003 | Batch 70250/114669 | Loss: 0.1355\n",
      "Epoch: 0003/0003 | Batch 70500/114669 | Loss: 0.3906\n",
      "Epoch: 0003/0003 | Batch 70750/114669 | Loss: 0.1077\n",
      "Epoch: 0003/0003 | Batch 71000/114669 | Loss: 0.0682\n",
      "Epoch: 0003/0003 | Batch 71250/114669 | Loss: 0.1893\n",
      "Epoch: 0003/0003 | Batch 71500/114669 | Loss: 0.7654\n",
      "Epoch: 0003/0003 | Batch 71750/114669 | Loss: 0.1432\n",
      "Epoch: 0003/0003 | Batch 72000/114669 | Loss: 0.5750\n",
      "Epoch: 0003/0003 | Batch 72250/114669 | Loss: 0.2565\n",
      "Epoch: 0003/0003 | Batch 72500/114669 | Loss: 0.1391\n",
      "Epoch: 0003/0003 | Batch 72750/114669 | Loss: 0.2331\n",
      "Epoch: 0003/0003 | Batch 73000/114669 | Loss: 0.4605\n",
      "Epoch: 0003/0003 | Batch 73250/114669 | Loss: 1.3154\n",
      "Epoch: 0003/0003 | Batch 73500/114669 | Loss: 0.2048\n",
      "Epoch: 0003/0003 | Batch 73750/114669 | Loss: 0.0887\n",
      "Epoch: 0003/0003 | Batch 74000/114669 | Loss: 0.0497\n",
      "Epoch: 0003/0003 | Batch 74250/114669 | Loss: 0.0431\n",
      "Epoch: 0003/0003 | Batch 74500/114669 | Loss: 0.1673\n",
      "Epoch: 0003/0003 | Batch 74750/114669 | Loss: 0.0914\n",
      "Epoch: 0003/0003 | Batch 75000/114669 | Loss: 0.1043\n",
      "Epoch: 0003/0003 | Batch 75250/114669 | Loss: 0.0942\n",
      "Epoch: 0003/0003 | Batch 75500/114669 | Loss: 0.3949\n",
      "Epoch: 0003/0003 | Batch 75750/114669 | Loss: 0.6411\n",
      "Epoch: 0003/0003 | Batch 76000/114669 | Loss: 0.1288\n",
      "Epoch: 0003/0003 | Batch 76250/114669 | Loss: 0.3654\n",
      "Epoch: 0003/0003 | Batch 76500/114669 | Loss: 0.2064\n",
      "Epoch: 0003/0003 | Batch 76750/114669 | Loss: 0.6669\n",
      "Epoch: 0003/0003 | Batch 77000/114669 | Loss: 0.1406\n",
      "Epoch: 0003/0003 | Batch 77250/114669 | Loss: 0.0702\n",
      "Epoch: 0003/0003 | Batch 77500/114669 | Loss: 0.2103\n",
      "Epoch: 0003/0003 | Batch 77750/114669 | Loss: 0.2774\n",
      "Epoch: 0003/0003 | Batch 78000/114669 | Loss: 0.0947\n",
      "Epoch: 0003/0003 | Batch 78250/114669 | Loss: 0.2790\n",
      "Epoch: 0003/0003 | Batch 78500/114669 | Loss: 0.4592\n",
      "Epoch: 0003/0003 | Batch 78750/114669 | Loss: 0.3913\n",
      "Epoch: 0003/0003 | Batch 79000/114669 | Loss: 0.0585\n",
      "Epoch: 0003/0003 | Batch 79250/114669 | Loss: 0.8867\n",
      "Epoch: 0003/0003 | Batch 79500/114669 | Loss: 0.1331\n",
      "Epoch: 0003/0003 | Batch 79750/114669 | Loss: 0.1542\n",
      "Epoch: 0003/0003 | Batch 80000/114669 | Loss: 0.2264\n",
      "Epoch: 0003/0003 | Batch 80250/114669 | Loss: 0.1077\n",
      "Epoch: 0003/0003 | Batch 80500/114669 | Loss: 0.0439\n",
      "Epoch: 0003/0003 | Batch 80750/114669 | Loss: 0.3130\n",
      "Epoch: 0003/0003 | Batch 81000/114669 | Loss: 0.0873\n",
      "Epoch: 0003/0003 | Batch 81250/114669 | Loss: 0.1302\n",
      "Epoch: 0003/0003 | Batch 81500/114669 | Loss: 0.3905\n",
      "Epoch: 0003/0003 | Batch 81750/114669 | Loss: 0.0462\n",
      "Epoch: 0003/0003 | Batch 82000/114669 | Loss: 0.6103\n",
      "Epoch: 0003/0003 | Batch 82250/114669 | Loss: 0.0478\n",
      "Epoch: 0003/0003 | Batch 82500/114669 | Loss: 0.4280\n",
      "Epoch: 0003/0003 | Batch 82750/114669 | Loss: 0.4513\n",
      "Epoch: 0003/0003 | Batch 83000/114669 | Loss: 0.1376\n",
      "Epoch: 0003/0003 | Batch 83250/114669 | Loss: 0.3404\n",
      "Epoch: 0003/0003 | Batch 83500/114669 | Loss: 0.0659\n",
      "Epoch: 0003/0003 | Batch 83750/114669 | Loss: 0.8231\n",
      "Epoch: 0003/0003 | Batch 84000/114669 | Loss: 0.0937\n",
      "Epoch: 0003/0003 | Batch 84250/114669 | Loss: 0.5079\n",
      "Epoch: 0003/0003 | Batch 84500/114669 | Loss: 0.4565\n",
      "Epoch: 0003/0003 | Batch 84750/114669 | Loss: 0.3143\n",
      "Epoch: 0003/0003 | Batch 85000/114669 | Loss: 0.0867\n",
      "Epoch: 0003/0003 | Batch 85250/114669 | Loss: 0.3355\n",
      "Epoch: 0003/0003 | Batch 85500/114669 | Loss: 0.0792\n",
      "Epoch: 0003/0003 | Batch 85750/114669 | Loss: 0.3176\n",
      "Epoch: 0003/0003 | Batch 86000/114669 | Loss: 0.1231\n",
      "Epoch: 0003/0003 | Batch 86250/114669 | Loss: 0.0697\n",
      "Epoch: 0003/0003 | Batch 86500/114669 | Loss: 0.0478\n",
      "Epoch: 0003/0003 | Batch 86750/114669 | Loss: 0.3319\n",
      "Epoch: 0003/0003 | Batch 87000/114669 | Loss: 0.4372\n",
      "Epoch: 0003/0003 | Batch 87250/114669 | Loss: 0.2878\n",
      "Epoch: 0003/0003 | Batch 87500/114669 | Loss: 0.2328\n",
      "Epoch: 0003/0003 | Batch 87750/114669 | Loss: 0.3642\n",
      "Epoch: 0003/0003 | Batch 88000/114669 | Loss: 0.3272\n",
      "Epoch: 0003/0003 | Batch 88250/114669 | Loss: 0.0575\n",
      "Epoch: 0003/0003 | Batch 88500/114669 | Loss: 0.2211\n",
      "Epoch: 0003/0003 | Batch 88750/114669 | Loss: 0.3107\n",
      "Epoch: 0003/0003 | Batch 89000/114669 | Loss: 0.1179\n",
      "Epoch: 0003/0003 | Batch 89250/114669 | Loss: 0.0280\n",
      "Epoch: 0003/0003 | Batch 89500/114669 | Loss: 0.2004\n",
      "Epoch: 0003/0003 | Batch 89750/114669 | Loss: 0.2145\n",
      "Epoch: 0003/0003 | Batch 90000/114669 | Loss: 0.4623\n",
      "Epoch: 0003/0003 | Batch 90250/114669 | Loss: 0.1694\n",
      "Epoch: 0003/0003 | Batch 90500/114669 | Loss: 0.3404\n",
      "Epoch: 0003/0003 | Batch 90750/114669 | Loss: 0.2747\n",
      "Epoch: 0003/0003 | Batch 91000/114669 | Loss: 0.1890\n",
      "Epoch: 0003/0003 | Batch 91250/114669 | Loss: 0.0501\n",
      "Epoch: 0003/0003 | Batch 91500/114669 | Loss: 0.0619\n",
      "Epoch: 0003/0003 | Batch 91750/114669 | Loss: 0.4083\n",
      "Epoch: 0003/0003 | Batch 92000/114669 | Loss: 0.4145\n",
      "Epoch: 0003/0003 | Batch 92250/114669 | Loss: 0.3412\n",
      "Epoch: 0003/0003 | Batch 92500/114669 | Loss: 0.1761\n",
      "Epoch: 0003/0003 | Batch 92750/114669 | Loss: 0.3327\n",
      "Epoch: 0003/0003 | Batch 93000/114669 | Loss: 0.0965\n",
      "Epoch: 0003/0003 | Batch 93250/114669 | Loss: 0.1087\n",
      "Epoch: 0003/0003 | Batch 93500/114669 | Loss: 0.2098\n",
      "Epoch: 0003/0003 | Batch 93750/114669 | Loss: 0.1280\n",
      "Epoch: 0003/0003 | Batch 94000/114669 | Loss: 0.2592\n",
      "Epoch: 0003/0003 | Batch 94250/114669 | Loss: 0.4893\n",
      "Epoch: 0003/0003 | Batch 94500/114669 | Loss: 0.3531\n",
      "Epoch: 0003/0003 | Batch 94750/114669 | Loss: 0.2583\n",
      "Epoch: 0003/0003 | Batch 95000/114669 | Loss: 0.5201\n",
      "Epoch: 0003/0003 | Batch 95250/114669 | Loss: 0.4011\n",
      "Epoch: 0003/0003 | Batch 95500/114669 | Loss: 0.4747\n",
      "Epoch: 0003/0003 | Batch 95750/114669 | Loss: 0.1203\n",
      "Epoch: 0003/0003 | Batch 96000/114669 | Loss: 0.0541\n",
      "Epoch: 0003/0003 | Batch 96250/114669 | Loss: 0.2530\n",
      "Epoch: 0003/0003 | Batch 96500/114669 | Loss: 0.1661\n",
      "Epoch: 0003/0003 | Batch 96750/114669 | Loss: 0.0942\n",
      "Epoch: 0003/0003 | Batch 97000/114669 | Loss: 0.9416\n",
      "Epoch: 0003/0003 | Batch 97250/114669 | Loss: 0.4444\n",
      "Epoch: 0003/0003 | Batch 97500/114669 | Loss: 0.1778\n",
      "Epoch: 0003/0003 | Batch 97750/114669 | Loss: 0.6022\n",
      "Epoch: 0003/0003 | Batch 98000/114669 | Loss: 0.3723\n",
      "Epoch: 0003/0003 | Batch 98250/114669 | Loss: 0.3138\n",
      "Epoch: 0003/0003 | Batch 98500/114669 | Loss: 0.0516\n",
      "Epoch: 0003/0003 | Batch 98750/114669 | Loss: 0.5865\n",
      "Epoch: 0003/0003 | Batch 99000/114669 | Loss: 0.0690\n",
      "Epoch: 0003/0003 | Batch 99250/114669 | Loss: 0.1629\n",
      "Epoch: 0003/0003 | Batch 99500/114669 | Loss: 0.1232\n",
      "Epoch: 0003/0003 | Batch 99750/114669 | Loss: 0.0818\n",
      "Epoch: 0003/0003 | Batch 100000/114669 | Loss: 0.1995\n",
      "Epoch: 0003/0003 | Batch 100250/114669 | Loss: 0.1376\n",
      "Epoch: 0003/0003 | Batch 100500/114669 | Loss: 0.0773\n",
      "Epoch: 0003/0003 | Batch 100750/114669 | Loss: 0.3312\n",
      "Epoch: 0003/0003 | Batch 101000/114669 | Loss: 0.0792\n",
      "Epoch: 0003/0003 | Batch 101250/114669 | Loss: 0.2622\n",
      "Epoch: 0003/0003 | Batch 101500/114669 | Loss: 0.0790\n",
      "Epoch: 0003/0003 | Batch 101750/114669 | Loss: 0.3420\n",
      "Epoch: 0003/0003 | Batch 102000/114669 | Loss: 0.2669\n",
      "Epoch: 0003/0003 | Batch 102250/114669 | Loss: 0.0698\n",
      "Epoch: 0003/0003 | Batch 102500/114669 | Loss: 0.3783\n",
      "Epoch: 0003/0003 | Batch 102750/114669 | Loss: 0.3437\n",
      "Epoch: 0003/0003 | Batch 103000/114669 | Loss: 0.3911\n",
      "Epoch: 0003/0003 | Batch 103250/114669 | Loss: 0.2310\n",
      "Epoch: 0003/0003 | Batch 103500/114669 | Loss: 0.3431\n",
      "Epoch: 0003/0003 | Batch 103750/114669 | Loss: 0.1133\n",
      "Epoch: 0003/0003 | Batch 104000/114669 | Loss: 0.1114\n",
      "Epoch: 0003/0003 | Batch 104250/114669 | Loss: 0.0164\n",
      "Epoch: 0003/0003 | Batch 104500/114669 | Loss: 0.3398\n",
      "Epoch: 0003/0003 | Batch 104750/114669 | Loss: 0.0574\n",
      "Epoch: 0003/0003 | Batch 105000/114669 | Loss: 0.3005\n",
      "Epoch: 0003/0003 | Batch 105250/114669 | Loss: 0.0768\n",
      "Epoch: 0003/0003 | Batch 105500/114669 | Loss: 0.1817\n",
      "Epoch: 0003/0003 | Batch 105750/114669 | Loss: 0.1798\n",
      "Epoch: 0003/0003 | Batch 106000/114669 | Loss: 0.0686\n",
      "Epoch: 0003/0003 | Batch 106250/114669 | Loss: 0.2580\n",
      "Epoch: 0003/0003 | Batch 106500/114669 | Loss: 0.3034\n",
      "Epoch: 0003/0003 | Batch 106750/114669 | Loss: 0.3739\n",
      "Epoch: 0003/0003 | Batch 107000/114669 | Loss: 0.4746\n",
      "Epoch: 0003/0003 | Batch 107250/114669 | Loss: 0.0942\n",
      "Epoch: 0003/0003 | Batch 107500/114669 | Loss: 0.2711\n",
      "Epoch: 0003/0003 | Batch 107750/114669 | Loss: 0.6396\n",
      "Epoch: 0003/0003 | Batch 108000/114669 | Loss: 0.2156\n",
      "Epoch: 0003/0003 | Batch 108250/114669 | Loss: 0.1354\n",
      "Epoch: 0003/0003 | Batch 108500/114669 | Loss: 0.1122\n",
      "Epoch: 0003/0003 | Batch 108750/114669 | Loss: 0.3909\n",
      "Epoch: 0003/0003 | Batch 109000/114669 | Loss: 0.1525\n",
      "Epoch: 0003/0003 | Batch 109250/114669 | Loss: 0.0926\n",
      "Epoch: 0003/0003 | Batch 109500/114669 | Loss: 0.0396\n",
      "Epoch: 0003/0003 | Batch 109750/114669 | Loss: 0.0855\n",
      "Epoch: 0003/0003 | Batch 110000/114669 | Loss: 0.0937\n",
      "Epoch: 0003/0003 | Batch 110250/114669 | Loss: 0.1069\n",
      "Epoch: 0003/0003 | Batch 110500/114669 | Loss: 0.1794\n",
      "Epoch: 0003/0003 | Batch 110750/114669 | Loss: 0.0617\n",
      "Epoch: 0003/0003 | Batch 111000/114669 | Loss: 0.3162\n",
      "Epoch: 0003/0003 | Batch 111250/114669 | Loss: 0.1682\n",
      "Epoch: 0003/0003 | Batch 111500/114669 | Loss: 0.5408\n",
      "Epoch: 0003/0003 | Batch 111750/114669 | Loss: 0.4821\n",
      "Epoch: 0003/0003 | Batch 112000/114669 | Loss: 0.8591\n",
      "Epoch: 0003/0003 | Batch 112250/114669 | Loss: 0.0514\n",
      "Epoch: 0003/0003 | Batch 112500/114669 | Loss: 0.5677\n",
      "Epoch: 0003/0003 | Batch 112750/114669 | Loss: 0.3448\n",
      "Epoch: 0003/0003 | Batch 113000/114669 | Loss: 0.3845\n",
      "Epoch: 0003/0003 | Batch 113250/114669 | Loss: 0.3788\n",
      "Epoch: 0003/0003 | Batch 113500/114669 | Loss: 0.2592\n",
      "Epoch: 0003/0003 | Batch 113750/114669 | Loss: 0.1662\n",
      "Epoch: 0003/0003 | Batch 114000/114669 | Loss: 0.4347\n",
      "Epoch: 0003/0003 | Batch 114250/114669 | Loss: 0.0939\n",
      "Epoch: 0003/0003 | Batch 114500/114669 | Loss: 0.2663\n",
      "Training accuracy: 31.91%\n",
      "Valid accuracy: 31.83%\n",
      "Time elapsed: 279.68 min\n",
      "Total Training Time: 279.68 min\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime elapsed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m60\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m min\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal Training Time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m60\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m min\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompute_accuracy(model,\u001b[38;5;250m \u001b[39mtest_loader,\u001b[38;5;250m \u001b[39mDEVICE)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_loader' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "RANDOM_SEED = 123\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "NUM_EPOCHS = 3\n",
    "\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        \n",
    "        ### Prepare data\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "        ### Forward\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss, logits = outputs['loss'], outputs['logits']\n",
    "        \n",
    "        ### Backward\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        ### Logging\n",
    "        if not batch_idx % 250:\n",
    "            print (f'Epoch: {epoch+1:04d}/{NUM_EPOCHS:04d} | '\n",
    "                   f'Batch {batch_idx:04d}/{len(train_loader):04d} | '\n",
    "                   f'Loss: {loss:.4f}')\n",
    "            \n",
    "    model.eval()\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        print(f'Training accuracy: '\n",
    "              f'{compute_accuracy(model, train_loader, DEVICE):.2f}%'\n",
    "              f'\\nValid accuracy: '\n",
    "              f'{compute_accuracy(model, valid_loader, DEVICE):.2f}%')\n",
    "        \n",
    "    print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')\n",
    "    \n",
    "print(f'Total Training Time: {(time.time() - start_time)/60:.2f} min')\n",
    "# print(f'Test accuracy: {compute_accuracy(model, test_loader, DEVICE):.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bda30e-b784-43c8-b776-4be7755bac2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92257c87-e087-4667-a317-8a387e0a214a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
